{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbdKO//Q9VQbkH6A/+8R8i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDH-teacher/Deep_Learning_Audit_code/blob/main/Normalization%2C_Optimization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C0bS4AkPyQlE"
      },
      "outputs": [],
      "source": [
        "# Normalization + Optimization (RNN) Colab runnable demo\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Reproducibility\n",
        "def set_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(0)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "# Toy text dataset (binary classification)\n",
        "# - label depends on the LAST token (so learning is visible)\n",
        "def build_vocab(vocab_size=200):\n",
        "    vocab_to_int = {\"<pad>\": 0}\n",
        "    for i in range(1, vocab_size):\n",
        "        vocab_to_int[f\"tok{i}\"] = i\n",
        "    return vocab_to_int\n",
        "\n",
        "vocab_to_int = build_vocab(vocab_size=200)\n",
        "\n",
        "def make_toy_sequences(n=800, seq_len=15, vocab_size=200, trigger_id=42, p_trigger=0.5, seed=0):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    X = rng.integers(1, vocab_size, size=(n, seq_len), dtype=np.int64)\n",
        "    y = np.zeros(n, dtype=np.int64)\n",
        "\n",
        "    mask = rng.random(n) < p_trigger\n",
        "    X[mask, -1] = trigger_id\n",
        "    y[mask] = 1\n",
        "\n",
        "    # make sure non-trigger samples don't accidentally end with trigger_id\n",
        "    if (~mask).sum() > 0:\n",
        "        last = rng.integers(1, vocab_size, size=((~mask).sum(),), dtype=np.int64)\n",
        "        last = np.where(last == trigger_id, trigger_id + 1, last)\n",
        "        X[~mask, -1] = last\n",
        "\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X, y = make_toy_sequences(n=800, seq_len=15, vocab_size=len(vocab_to_int), seed=0)\n",
        "\n",
        "# split: train/dev/test\n",
        "n = len(X)\n",
        "n_train = int(n * 0.7)\n",
        "n_dev = int(n * 0.15)\n",
        "\n",
        "X_train, y_train = X[:n_train], y[:n_train]\n",
        "X_dev,   y_dev   = X[n_train:n_train + n_dev], y[n_train:n_train + n_dev]\n",
        "X_test,  y_test  = X[n_train + n_dev:], y[n_train + n_dev:]\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.long)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.X.size(0)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_ids = self.X[idx]\n",
        "        attention_masks = (input_ids != 0).long()\n",
        "        segment_ids = torch.zeros_like(input_ids)\n",
        "        labels = self.y[idx]\n",
        "        return input_ids, attention_masks, segment_ids, labels\n",
        "\n",
        "train_loader = DataLoader(TextDataset(X_train, y_train), batch_size=64, shuffle=True)\n",
        "dev_loader   = DataLoader(TextDataset(X_dev, y_dev),     batch_size=128, shuffle=False)\n",
        "test_loader  = DataLoader(TextDataset(X_test, y_test),   batch_size=128, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN + (BatchNorm or LayerNorm) before RNN forward\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        # Define Embedding\n",
        "        self.embedding = nn.Embedding(len(vocab_to_int), input_size, padding_idx=vocab_to_int['<pad>'])\n",
        "\n",
        "        # Define batch normalization\n",
        "        self.bn = nn.BatchNorm1d(input_size)\n",
        "\n",
        "        # Define layer normalization\n",
        "        self.ln = nn.LayerNorm(input_size)\n",
        "\n",
        "        # Define the RNN layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Define the fully connected layer to produce outputs\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        # choose normalization mode: \"bn\" / \"ln\" / None\n",
        "        self.norm_mode = \"ln\"\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embedding(x)\n",
        "\n",
        "        ## Batch Normalization\n",
        "        if self.norm_mode == \"bn\":\n",
        "            embedding = embedding.permute(0, 2, 1)\n",
        "            embedding = self.bn(embedding).permute(0, 2, 1)\n",
        "\n",
        "        ## Layer Normalization\n",
        "        if self.norm_mode == \"ln\":\n",
        "            embedding = self.ln(embedding)\n",
        "\n",
        "        # Forward propagate the RNN\n",
        "        out, hidden = self.rnn(embedding)\n",
        "\n",
        "        # Take the output from the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "\n",
        "# Train / Eval (Optimization + Monitoring logs)\n",
        "def accuracy_percent(logits, labels):\n",
        "    preds = logits.argmax(dim=1)\n",
        "    return (preds == labels).float().mean().item() * 100.0\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(loader, model, criterion):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "    accs = []\n",
        "    for data in loader:\n",
        "        input_ids, attention_masks, segment_ids, labels = data\n",
        "        input_ids = input_ids.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        out = model(input_ids)\n",
        "        loss = criterion(out, labels)\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        accs.append(accuracy_percent(out, labels))\n",
        "\n",
        "    return float(np.mean(losses)), float(np.mean(accs))"
      ],
      "metadata": {
        "id": "x02YR3XpyTcO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_experiment(\n",
        "    title: str,\n",
        "    lr: float,\n",
        "    epochs: int,\n",
        "    norm_mode: str = \"ln\",\n",
        "    max_grad_norm: float = 0.0,\n",
        "    stop_if_loss_gt: float = 1e8):\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"{title} | lr={lr:g} | norm={norm_mode} | max_grad_norm={max_grad_norm}\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    set_seed(0)\n",
        "    model = RNNModel(input_size=10, hidden_size=20, num_layers=2, output_size=2).to(device)\n",
        "    model.norm_mode = norm_mode\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        train_accs = []\n",
        "\n",
        "        # for data in loader: ...\n",
        "        for data in train_loader:\n",
        "            input_ids, attention_masks, segment_ids, labels = data\n",
        "            input_ids = input_ids.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            out = model(input_ids)\n",
        "            loss = criterion(out, labels)\n",
        "\n",
        "            if (not torch.isfinite(loss)) or (loss.item() > stop_if_loss_gt):\n",
        "                print(f\"{epoch} epoch, train_loss = {loss.item():.6f}  -> (stopped early: non-finite or exploding)\")\n",
        "                return\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping\n",
        "            if max_grad_norm > 0.:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_losses.append(loss.item())\n",
        "            train_accs.append(accuracy_percent(out, labels))\n",
        "\n",
        "        train_loss = float(np.mean(train_losses))\n",
        "        train_acc = float(np.mean(train_accs))\n",
        "        eval_loss, eval_acc = evaluate(dev_loader, model, criterion)\n",
        "\n",
        "        print(f\"{epoch} epoch, train_loss = {train_loss:.6f}, train_acc:{train_acc:.1f}, eval_loss:{eval_loss:.6f}, eval_acc:{eval_acc:.1f}\")\n",
        "\n",
        "        if (not np.isfinite(train_loss)) or (train_loss > stop_if_loss_gt):\n",
        "            print(\"(stopped early: average loss exploded)\")\n",
        "            return\n",
        "\n",
        "    test_loss, test_acc = evaluate(test_loader, model, criterion)\n",
        "    print(f\"[FINAL TEST] loss:{test_loss:.6f}, acc:{test_acc:.1f}\")"
      ],
      "metadata": {
        "id": "tafiRwT-yTaW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss barely changing: lr too small\n",
        "train_one_experiment(\n",
        "    title=\"CASE 1) LR too small -> loss barely changes\",\n",
        "    lr=1e-5,\n",
        "    epochs=35,\n",
        "    norm_mode=\"ln\",\n",
        "    max_grad_norm=0.0,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0uYcjX2CyTWC",
        "outputId": "b7fc9302-7170-4a13-b929-b1da99d1a38f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CASE 1) LR too small -> loss barely changes | lr=1e-05 | norm=ln | max_grad_norm=0.0\n",
            "================================================================================\n",
            "1 epoch, train_loss = 0.810851, train_acc:38.7, eval_loss:0.798933, eval_acc:40.8\n",
            "2 epoch, train_loss = 0.809332, train_acc:38.9, eval_loss:0.798894, eval_acc:40.8\n",
            "3 epoch, train_loss = 0.809024, train_acc:39.1, eval_loss:0.798854, eval_acc:40.8\n",
            "4 epoch, train_loss = 0.810841, train_acc:38.7, eval_loss:0.798814, eval_acc:40.8\n",
            "5 epoch, train_loss = 0.810063, train_acc:38.7, eval_loss:0.798774, eval_acc:40.8\n",
            "6 epoch, train_loss = 0.810356, train_acc:38.7, eval_loss:0.798735, eval_acc:40.8\n",
            "7 epoch, train_loss = 0.808025, train_acc:39.2, eval_loss:0.798696, eval_acc:40.8\n",
            "8 epoch, train_loss = 0.808075, train_acc:39.2, eval_loss:0.798656, eval_acc:40.8\n",
            "9 epoch, train_loss = 0.809855, train_acc:38.8, eval_loss:0.798617, eval_acc:40.8\n",
            "10 epoch, train_loss = 0.808708, train_acc:39.1, eval_loss:0.798577, eval_acc:40.8\n",
            "11 epoch, train_loss = 0.808219, train_acc:39.2, eval_loss:0.798538, eval_acc:40.8\n",
            "12 epoch, train_loss = 0.808065, train_acc:39.1, eval_loss:0.798499, eval_acc:40.8\n",
            "13 epoch, train_loss = 0.808949, train_acc:38.8, eval_loss:0.798459, eval_acc:40.8\n",
            "14 epoch, train_loss = 0.809635, train_acc:38.8, eval_loss:0.798420, eval_acc:40.8\n",
            "15 epoch, train_loss = 0.808158, train_acc:39.2, eval_loss:0.798381, eval_acc:40.8\n",
            "16 epoch, train_loss = 0.809798, train_acc:38.9, eval_loss:0.798341, eval_acc:40.8\n",
            "17 epoch, train_loss = 0.810601, train_acc:38.6, eval_loss:0.798302, eval_acc:40.8\n",
            "18 epoch, train_loss = 0.808842, train_acc:38.9, eval_loss:0.798262, eval_acc:40.8\n",
            "19 epoch, train_loss = 0.807883, train_acc:39.2, eval_loss:0.798223, eval_acc:40.8\n",
            "20 epoch, train_loss = 0.810059, train_acc:38.5, eval_loss:0.798184, eval_acc:40.8\n",
            "21 epoch, train_loss = 0.808777, train_acc:38.9, eval_loss:0.798144, eval_acc:40.8\n",
            "22 epoch, train_loss = 0.808501, train_acc:39.0, eval_loss:0.798105, eval_acc:40.8\n",
            "23 epoch, train_loss = 0.809000, train_acc:38.9, eval_loss:0.798065, eval_acc:40.8\n",
            "24 epoch, train_loss = 0.809364, train_acc:38.9, eval_loss:0.798026, eval_acc:40.8\n",
            "25 epoch, train_loss = 0.808968, train_acc:38.7, eval_loss:0.797986, eval_acc:40.8\n",
            "26 epoch, train_loss = 0.808052, train_acc:38.9, eval_loss:0.797947, eval_acc:40.8\n",
            "27 epoch, train_loss = 0.809912, train_acc:38.6, eval_loss:0.797908, eval_acc:40.8\n",
            "28 epoch, train_loss = 0.807979, train_acc:39.1, eval_loss:0.797869, eval_acc:40.8\n",
            "29 epoch, train_loss = 0.808958, train_acc:38.9, eval_loss:0.797829, eval_acc:40.8\n",
            "30 epoch, train_loss = 0.807433, train_acc:39.1, eval_loss:0.797790, eval_acc:40.8\n",
            "31 epoch, train_loss = 0.807808, train_acc:39.2, eval_loss:0.797751, eval_acc:40.8\n",
            "32 epoch, train_loss = 0.808434, train_acc:38.9, eval_loss:0.797712, eval_acc:40.8\n",
            "33 epoch, train_loss = 0.809136, train_acc:38.7, eval_loss:0.797672, eval_acc:40.8\n",
            "34 epoch, train_loss = 0.808459, train_acc:38.9, eval_loss:0.797633, eval_acc:40.8\n",
            "35 epoch, train_loss = 0.807280, train_acc:39.2, eval_loss:0.797594, eval_acc:40.8\n",
            "[FINAL TEST] loss:0.795785, acc:41.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LR too large -> loss explodes\n",
        "train_one_experiment(\n",
        "    title=\"CASE 2) LR too large -> loss explodes (stop before NaN)\",\n",
        "    lr=1e3,\n",
        "    epochs=15,\n",
        "    norm_mode=\"ln\",\n",
        "    max_grad_norm=0.0,\n",
        "    stop_if_loss_gt=1e6,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRx2owPUziWN",
        "outputId": "da8ed409-0c2e-4061-81ab-263a9cbfb437"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CASE 2) LR too large -> loss explodes (stop before NaN) | lr=1000 | norm=ln | max_grad_norm=0.0\n",
            "================================================================================\n",
            "1 epoch, train_loss = 3492.913072, train_acc:51.7, eval_loss:7009.353027, eval_acc:50.8\n",
            "2 epoch, train_loss = 4956.795519, train_acc:50.8, eval_loss:9199.971680, eval_acc:50.0\n",
            "3 epoch, train_loss = 5993.430664, train_acc:48.1, eval_loss:4981.220215, eval_acc:50.0\n",
            "4 epoch, train_loss = 5732.435791, train_acc:48.8, eval_loss:1449.972412, eval_acc:50.0\n",
            "5 epoch, train_loss = 4616.103638, train_acc:53.2, eval_loss:2200.235352, eval_acc:50.8\n",
            "6 epoch, train_loss = 4872.315111, train_acc:51.6, eval_loss:5641.900879, eval_acc:50.8\n",
            "7 epoch, train_loss = 4441.189134, train_acc:52.4, eval_loss:10206.578125, eval_acc:50.0\n",
            "8 epoch, train_loss = 5686.153388, train_acc:50.2, eval_loss:5731.222168, eval_acc:50.0\n",
            "9 epoch, train_loss = 5239.835626, train_acc:51.0, eval_loss:1762.473755, eval_acc:50.0\n",
            "10 epoch, train_loss = 6167.294908, train_acc:45.9, eval_loss:2676.535645, eval_acc:50.8\n",
            "11 epoch, train_loss = 5266.949775, train_acc:49.1, eval_loss:6906.916992, eval_acc:50.8\n",
            "12 epoch, train_loss = 4583.882521, train_acc:51.2, eval_loss:11376.522461, eval_acc:50.8\n",
            "13 epoch, train_loss = 6108.976657, train_acc:49.2, eval_loss:4399.402832, eval_acc:50.0\n",
            "14 epoch, train_loss = 5191.839091, train_acc:51.1, eval_loss:52.825905, eval_acc:50.8\n",
            "15 epoch, train_loss = 5351.322557, train_acc:49.2, eval_loss:4785.116211, eval_acc:50.8\n",
            "[FINAL TEST] loss:4947.323730, acc:49.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reasonable lr -> learning works\n",
        "train_one_experiment(\n",
        "    title=\"CASE 3) LR=1e-3 -> learning works\",\n",
        "    lr=1e-3,\n",
        "    epochs=100,\n",
        "    norm_mode=\"ln\",\n",
        "    max_grad_norm=0.0,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcw2Dm-DziRe",
        "outputId": "08648e06-558c-4439-b60d-52eab65fd35d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "CASE 3) LR=1e-3 -> learning works | lr=0.001 | norm=ln | max_grad_norm=0.0\n",
            "================================================================================\n",
            "1 epoch, train_loss = 0.808923, train_acc:38.7, eval_loss:0.795032, eval_acc:40.0\n",
            "2 epoch, train_loss = 0.803179, train_acc:38.6, eval_loss:0.791240, eval_acc:40.0\n",
            "3 epoch, train_loss = 0.798767, train_acc:38.5, eval_loss:0.787550, eval_acc:40.0\n",
            "4 epoch, train_loss = 0.796386, train_acc:38.0, eval_loss:0.783918, eval_acc:40.0\n",
            "5 epoch, train_loss = 0.791721, train_acc:38.0, eval_loss:0.780402, eval_acc:40.0\n",
            "6 epoch, train_loss = 0.788136, train_acc:37.7, eval_loss:0.776972, eval_acc:40.0\n",
            "7 epoch, train_loss = 0.782601, train_acc:38.1, eval_loss:0.773693, eval_acc:40.0\n",
            "8 epoch, train_loss = 0.779080, train_acc:37.7, eval_loss:0.770491, eval_acc:40.0\n",
            "9 epoch, train_loss = 0.776995, train_acc:36.9, eval_loss:0.767334, eval_acc:40.0\n",
            "10 epoch, train_loss = 0.772754, train_acc:37.0, eval_loss:0.764276, eval_acc:40.0\n",
            "11 epoch, train_loss = 0.769076, train_acc:36.9, eval_loss:0.761304, eval_acc:40.8\n",
            "12 epoch, train_loss = 0.765844, train_acc:36.5, eval_loss:0.758409, eval_acc:40.0\n",
            "13 epoch, train_loss = 0.763324, train_acc:35.8, eval_loss:0.755566, eval_acc:40.0\n",
            "14 epoch, train_loss = 0.760737, train_acc:35.4, eval_loss:0.752778, eval_acc:40.0\n",
            "15 epoch, train_loss = 0.756808, train_acc:35.4, eval_loss:0.750084, eval_acc:40.0\n",
            "16 epoch, train_loss = 0.754896, train_acc:34.4, eval_loss:0.747426, eval_acc:40.0\n",
            "17 epoch, train_loss = 0.752362, train_acc:34.5, eval_loss:0.744806, eval_acc:37.5\n",
            "18 epoch, train_loss = 0.748580, train_acc:34.2, eval_loss:0.742274, eval_acc:37.5\n",
            "19 epoch, train_loss = 0.745378, train_acc:34.2, eval_loss:0.739816, eval_acc:37.5\n",
            "20 epoch, train_loss = 0.743897, train_acc:33.8, eval_loss:0.737373, eval_acc:37.5\n",
            "21 epoch, train_loss = 0.740244, train_acc:34.0, eval_loss:0.734995, eval_acc:37.5\n",
            "22 epoch, train_loss = 0.737641, train_acc:33.9, eval_loss:0.732676, eval_acc:37.5\n",
            "23 epoch, train_loss = 0.735505, train_acc:33.2, eval_loss:0.730398, eval_acc:37.5\n",
            "24 epoch, train_loss = 0.733017, train_acc:33.2, eval_loss:0.728159, eval_acc:35.8\n",
            "25 epoch, train_loss = 0.730468, train_acc:32.9, eval_loss:0.725970, eval_acc:36.7\n",
            "26 epoch, train_loss = 0.727722, train_acc:32.8, eval_loss:0.723832, eval_acc:35.8\n",
            "27 epoch, train_loss = 0.726320, train_acc:32.3, eval_loss:0.721716, eval_acc:35.8\n",
            "28 epoch, train_loss = 0.723308, train_acc:32.8, eval_loss:0.719667, eval_acc:35.8\n",
            "29 epoch, train_loss = 0.721287, train_acc:32.8, eval_loss:0.717636, eval_acc:35.0\n",
            "30 epoch, train_loss = 0.718622, train_acc:33.6, eval_loss:0.715661, eval_acc:35.8\n",
            "31 epoch, train_loss = 0.716600, train_acc:34.0, eval_loss:0.713716, eval_acc:35.8\n",
            "32 epoch, train_loss = 0.714869, train_acc:34.4, eval_loss:0.711802, eval_acc:35.8\n",
            "33 epoch, train_loss = 0.712658, train_acc:35.0, eval_loss:0.709904, eval_acc:37.5\n",
            "34 epoch, train_loss = 0.710590, train_acc:35.8, eval_loss:0.708051, eval_acc:37.5\n",
            "35 epoch, train_loss = 0.708245, train_acc:36.8, eval_loss:0.706236, eval_acc:37.5\n",
            "36 epoch, train_loss = 0.706566, train_acc:37.0, eval_loss:0.704457, eval_acc:37.5\n",
            "37 epoch, train_loss = 0.704392, train_acc:38.7, eval_loss:0.702686, eval_acc:38.3\n",
            "38 epoch, train_loss = 0.702231, train_acc:40.6, eval_loss:0.700948, eval_acc:43.3\n",
            "39 epoch, train_loss = 0.700496, train_acc:42.4, eval_loss:0.699237, eval_acc:44.2\n",
            "40 epoch, train_loss = 0.698967, train_acc:44.2, eval_loss:0.697541, eval_acc:45.8\n",
            "41 epoch, train_loss = 0.697280, train_acc:45.6, eval_loss:0.695868, eval_acc:48.3\n",
            "42 epoch, train_loss = 0.695343, train_acc:48.5, eval_loss:0.694230, eval_acc:50.8\n",
            "43 epoch, train_loss = 0.693366, train_acc:50.4, eval_loss:0.692601, eval_acc:56.7\n",
            "44 epoch, train_loss = 0.691911, train_acc:51.9, eval_loss:0.690998, eval_acc:58.3\n",
            "45 epoch, train_loss = 0.690083, train_acc:53.8, eval_loss:0.689425, eval_acc:57.5\n",
            "46 epoch, train_loss = 0.688750, train_acc:54.9, eval_loss:0.687881, eval_acc:60.0\n",
            "47 epoch, train_loss = 0.686682, train_acc:56.8, eval_loss:0.686362, eval_acc:60.0\n",
            "48 epoch, train_loss = 0.685137, train_acc:57.8, eval_loss:0.684847, eval_acc:61.7\n",
            "49 epoch, train_loss = 0.683541, train_acc:59.3, eval_loss:0.683346, eval_acc:62.5\n",
            "50 epoch, train_loss = 0.682076, train_acc:60.3, eval_loss:0.681867, eval_acc:62.5\n",
            "51 epoch, train_loss = 0.680405, train_acc:61.5, eval_loss:0.680398, eval_acc:64.2\n",
            "52 epoch, train_loss = 0.679239, train_acc:62.5, eval_loss:0.678953, eval_acc:68.3\n",
            "53 epoch, train_loss = 0.677278, train_acc:63.9, eval_loss:0.677511, eval_acc:69.2\n",
            "54 epoch, train_loss = 0.675762, train_acc:65.9, eval_loss:0.676081, eval_acc:70.0\n",
            "55 epoch, train_loss = 0.673991, train_acc:67.8, eval_loss:0.674670, eval_acc:72.5\n",
            "56 epoch, train_loss = 0.672500, train_acc:68.8, eval_loss:0.673268, eval_acc:71.7\n",
            "57 epoch, train_loss = 0.671577, train_acc:69.1, eval_loss:0.671891, eval_acc:71.7\n",
            "58 epoch, train_loss = 0.669980, train_acc:70.2, eval_loss:0.670515, eval_acc:72.5\n",
            "59 epoch, train_loss = 0.668393, train_acc:71.3, eval_loss:0.669160, eval_acc:72.5\n",
            "60 epoch, train_loss = 0.666817, train_acc:71.6, eval_loss:0.667811, eval_acc:73.3\n",
            "61 epoch, train_loss = 0.665335, train_acc:71.6, eval_loss:0.666479, eval_acc:73.3\n",
            "62 epoch, train_loss = 0.663924, train_acc:72.6, eval_loss:0.665147, eval_acc:73.3\n",
            "63 epoch, train_loss = 0.662451, train_acc:72.8, eval_loss:0.663828, eval_acc:72.5\n",
            "64 epoch, train_loss = 0.661397, train_acc:72.8, eval_loss:0.662526, eval_acc:72.5\n",
            "65 epoch, train_loss = 0.659219, train_acc:73.2, eval_loss:0.661225, eval_acc:73.3\n",
            "66 epoch, train_loss = 0.658445, train_acc:73.4, eval_loss:0.659936, eval_acc:73.3\n",
            "67 epoch, train_loss = 0.656961, train_acc:73.9, eval_loss:0.658662, eval_acc:74.2\n",
            "68 epoch, train_loss = 0.655561, train_acc:74.3, eval_loss:0.657386, eval_acc:74.2\n",
            "69 epoch, train_loss = 0.654867, train_acc:73.7, eval_loss:0.656127, eval_acc:75.0\n",
            "70 epoch, train_loss = 0.652897, train_acc:74.1, eval_loss:0.654869, eval_acc:74.2\n",
            "71 epoch, train_loss = 0.651462, train_acc:74.0, eval_loss:0.653616, eval_acc:74.2\n",
            "72 epoch, train_loss = 0.650154, train_acc:74.2, eval_loss:0.652365, eval_acc:74.2\n",
            "73 epoch, train_loss = 0.649361, train_acc:74.0, eval_loss:0.651126, eval_acc:74.2\n",
            "74 epoch, train_loss = 0.647771, train_acc:74.4, eval_loss:0.649893, eval_acc:74.2\n",
            "75 epoch, train_loss = 0.646720, train_acc:74.4, eval_loss:0.648673, eval_acc:74.2\n",
            "76 epoch, train_loss = 0.644357, train_acc:74.8, eval_loss:0.647440, eval_acc:74.2\n",
            "77 epoch, train_loss = 0.644006, train_acc:74.2, eval_loss:0.646220, eval_acc:75.0\n",
            "78 epoch, train_loss = 0.642839, train_acc:74.2, eval_loss:0.645005, eval_acc:74.2\n",
            "79 epoch, train_loss = 0.641364, train_acc:74.1, eval_loss:0.643801, eval_acc:74.2\n",
            "80 epoch, train_loss = 0.640620, train_acc:74.2, eval_loss:0.642600, eval_acc:74.2\n",
            "81 epoch, train_loss = 0.639750, train_acc:74.1, eval_loss:0.641410, eval_acc:74.2\n",
            "82 epoch, train_loss = 0.637774, train_acc:74.4, eval_loss:0.640211, eval_acc:75.0\n",
            "83 epoch, train_loss = 0.636868, train_acc:74.2, eval_loss:0.639018, eval_acc:75.0\n",
            "84 epoch, train_loss = 0.634722, train_acc:74.5, eval_loss:0.637830, eval_acc:75.0\n",
            "85 epoch, train_loss = 0.634563, train_acc:74.1, eval_loss:0.636647, eval_acc:75.0\n",
            "86 epoch, train_loss = 0.632324, train_acc:74.7, eval_loss:0.635460, eval_acc:74.2\n",
            "87 epoch, train_loss = 0.631654, train_acc:74.1, eval_loss:0.634284, eval_acc:74.2\n",
            "88 epoch, train_loss = 0.630417, train_acc:74.3, eval_loss:0.633108, eval_acc:74.2\n",
            "89 epoch, train_loss = 0.629301, train_acc:74.4, eval_loss:0.631931, eval_acc:74.2\n",
            "90 epoch, train_loss = 0.627892, train_acc:74.4, eval_loss:0.630763, eval_acc:74.2\n",
            "91 epoch, train_loss = 0.626742, train_acc:74.5, eval_loss:0.629595, eval_acc:75.0\n",
            "92 epoch, train_loss = 0.625518, train_acc:74.6, eval_loss:0.628431, eval_acc:75.0\n",
            "93 epoch, train_loss = 0.624631, train_acc:74.4, eval_loss:0.627263, eval_acc:75.0\n",
            "94 epoch, train_loss = 0.623091, train_acc:74.5, eval_loss:0.626099, eval_acc:75.0\n",
            "95 epoch, train_loss = 0.621067, train_acc:74.8, eval_loss:0.624929, eval_acc:74.2\n",
            "96 epoch, train_loss = 0.620626, train_acc:74.8, eval_loss:0.623762, eval_acc:74.2\n",
            "97 epoch, train_loss = 0.619398, train_acc:74.8, eval_loss:0.622594, eval_acc:74.2\n",
            "98 epoch, train_loss = 0.618554, train_acc:74.6, eval_loss:0.621433, eval_acc:75.0\n",
            "99 epoch, train_loss = 0.616268, train_acc:75.0, eval_loss:0.620270, eval_acc:75.0\n",
            "100 epoch, train_loss = 0.616610, train_acc:74.4, eval_loss:0.619110, eval_acc:75.0\n",
            "[FINAL TEST] loss:0.611495, acc:78.3\n"
          ]
        }
      ]
    }
  ]
}