{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDH-teacher/Deep_Learning_Audit_code/blob/main/LoRA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUR9N70DrzRQ",
        "outputId": "310c5be6-d2ba-437b-a38d-7dc12a5c3557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib\n",
        "!pip install -q sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDjLZzqOsLU_",
        "outputId": "96efd834-7b98-4d32-cf8b-3c71237af278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "data = load_dataset(\"squad\")\n",
        "\n",
        "model_name = \"bigscience/bloom-560m\"\n",
        "\n",
        "lora_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "if lora_tokenizer.pad_token is None:\n",
        "    lora_tokenizer.pad_token = lora_tokenizer.eos_token\n",
        "\n",
        "def create_prompt(context, question, answers):\n",
        "    answer_text = answers[\"text\"][0] if len(answers[\"text\"]) > 0 else \"\"\n",
        "    prompt = (\n",
        "        f\"### Context: {context}\\n\"\n",
        "        f\"### Question: {question}\\n\"\n",
        "        f\"### Answer: {answer_text}\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "train_small = data[\"train\"].select(range(200))\n",
        "val_small   = data[\"validation\"].select(range(50))\n",
        "\n",
        "print(\"train_small:\", len(train_small), \"val_small:\", len(val_small))\n",
        "print(\"sample answers:\", train_small[0][\"answers\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183,
          "referenced_widgets": [
            "068c05a67c3d424e810cfb07e1019d1b",
            "efadf674ce834a63badc4bccaf9f7d9a",
            "12b636df9b54493aaeafb54f308a17fe",
            "e825dd516f5c477cbd76c0a41c94d0d8",
            "0a9541fdbe9f482cb1f5962691ece84b",
            "fb4f8a5f162b497ebcedbeb2777ff2ee",
            "059ba06472274f7f9440164686d70afd",
            "dddb61b5c44b4be5a85eb91b7ea11e3e",
            "64705eed50814171bf2860de4d991ee4",
            "d4d4c4d5ccaa4b6a8ab0ca089b43a376",
            "92185b3dd15345a6a6582dc5c1819835",
            "e4489545de3e4826b7e97d22ca1fca06",
            "4cc3a64910d74f05ba3a90fcc26ee44a",
            "a2e0f540dce542f197a89733b355b5e1",
            "cca0f68f5e9c4f71a166213619d3409c",
            "2484bc7e6e3646a9b8ff84324539ac12",
            "bd36741db05348b1a8c52ad3153f4a4d",
            "41c1c479f358460b9762686f3ff7665a",
            "c0a266614ad44c94a7a213be358eaa3d",
            "608bc01ad9cd4a12bc1a026c76793a18",
            "76af37d2eb5644dc92b7653d662fbc0f",
            "725760e185da4753aac38dc5c5f73e61",
            "02f3e92cd8d14bc680eae5c3c8824932",
            "02f9c7649c0a48acb19bc475d726bea1",
            "4ef2a12bb63a4f87b343458a31d39028",
            "9521f2ca93d5434896e365082f966f05",
            "eb9e0b9361b24664a3dfa58703844845",
            "0ed596b0223942a28622c1aa2cf0ba92",
            "d2d4c40ba8c04745b5f50ff4442f7cc3",
            "85fcca33236346009943a1e79f0f3fe3",
            "2cf1cfdbf94a4b2c8993cadfa29bc8d8",
            "ec177f19d185406a8e2209b8ce2e2e20",
            "91b3374c96ad45978d324fd7721d541d",
            "a9d9cfd8f8fa4e21824f2da8c284506e",
            "f63d8021fbcd45f7860fb67ed9f5341a",
            "f73a8fe161814410a1efd7829f9d64f1",
            "9be9c7a63def464abda1ed66df9c91ef",
            "5bac30ca6da946799eb23273581e6fa8",
            "4f98f7306c444dfdb219bf2f6da8cbb7",
            "a0f8078b26464fe58ac960aa0f4bc06c",
            "16c6816ec2b54bcba2de0e885ef61d51",
            "3cc26033a4704f7da4d5a0e9caf0bd33",
            "56380f50828d464a9766d175d9f85c2e",
            "6526a594acc1487ca3932e88ae4a946a"
          ]
        },
        "id": "gFFT2xJ0sLTG",
        "outputId": "a265d313-414e-4d36-ddba-c0b4bbd8930e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/693 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "068c05a67c3d424e810cfb07e1019d1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/222 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4489545de3e4826b7e97d22ca1fca06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "02f3e92cd8d14bc680eae5c3c8824932"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9d9cfd8f8fa4e21824f2da8c284506e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_small: 200 val_small: 50\n",
            "sample answers: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 256\n",
        "\n",
        "def tok_fn(ex):\n",
        "    prompt = create_prompt(ex[\"context\"], ex[\"question\"], ex[\"answers\"])\n",
        "    return lora_tokenizer(\n",
        "        prompt,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN,\n",
        "    )\n",
        "\n",
        "tokenized_train = train_small.map(tok_fn, remove_columns=train_small.column_names)\n",
        "tokenized_val   = val_small.map(tok_fn, remove_columns=val_small.column_names)\n",
        "\n",
        "print(tokenized_train[0].keys())\n",
        "print(tokenized_train[0][\"input_ids\"][:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "e8061e395bc54d32937f75bd81905410",
            "939373c5ad374006bd7cd88f1a6da6cb",
            "5e7f3709b03d4702b6a36f7ba7301d22",
            "058df835aff040c5a3b9a22340101556",
            "999df7138299458c8e76eb932925d053",
            "88e40792278044b7ac47b046dd2ccc82",
            "9a1b9d0e971248829832581b83f0ad3e",
            "e3bd06670ba3450eafe9d08418132531",
            "2a1b1744c1c04ca1937180622b39efcb",
            "4014a08b588048cfb7a9ec259e2cf100",
            "67490ba875e143bb988bdc22dc781843",
            "6d75ab3a4a1940f2853612d26dd6970b",
            "9d4615a085b249828c8ec30c9800ac05",
            "6be9cb84cedb4518b2ed29e76203f54e",
            "3e5ab54250924c42b9264aa4ffc1ce23",
            "7242ee460f874a7bbecc30954ae2055a",
            "615b92ddfd2b45bab0637bab967ea142",
            "804c68f206b04bad85501ced1ac1124e",
            "54bbc4186a094ed0901f2a62b777829d",
            "237ea569c5ac4594832c8723ee3927ac",
            "9e376a6010c649d0b82a40e49cb65587",
            "51e103e64c6b4097a10816a6f34a2700"
          ]
        },
        "id": "LO2F1bZIvFrH",
        "outputId": "a840ce4d-0527-41e0-c488-1498cee8669b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8061e395bc54d32937f75bd81905410"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d75ab3a4a1940f2853612d26dd6970b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_keys(['input_ids', 'attention_mask'])\n",
            "[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    target_modules=[\"query_key_value\"],\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "d33706a9c4c141489c3a60db425d2014",
            "38f4342d9b5b4666b82b5a1f34545703",
            "b80b5d5a192949f0aa285b49df48c338",
            "ede8ba373f124257aaaa5a42c4ffb9b8",
            "08809ded1c63486f949300c68c6e7c5c",
            "8d9546f3f3e7448f9240d41587d52d21",
            "49c509c93f2547bd8a8023c6d1ee6fe8",
            "b6b8a5c33be745c7a79e4e9356e5eb5f",
            "f5ea09d392324a42825d515b52d0fa51",
            "ab1dcd7f41244358b01656cdd5821e18",
            "b45954c8a4c048e4893f360e64c85a90",
            "c0b9630215634c3bb0e5b7ed70bfc72e",
            "bbec56949b13404ba94b39680497a7b7",
            "fa8767194c5e42ce879b10601810ac51",
            "8447887ea12c4052a4f89dab6638eda3",
            "fd8ca46a6a1a4ca594155528ddd0a510",
            "6571790f30e248d5aec1a9c01a6451e7",
            "a137a0c73633474795f4fe4af50588ff",
            "aee31a7b88a148d2b0f744219f729ea2",
            "4fb79cc26dac46e49ea487c71688dfdb",
            "9907d733d1784fb3910685ef51dbb6a1",
            "77823616268e4da7894933b4de1f2732"
          ]
        },
        "id": "LUCPs54KsK7O",
        "outputId": "f3af8921-7cc1-4d71-beef-64e38e176c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`torch_dtype` is deprecated! Use `dtype` instead!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d33706a9c4c141489c3a60db425d2014"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading weights:   0%|          | 0/293 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0b9630215634c3bb0e5b7ed70bfc72e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,572,864 || all params: 560,787,456 || trainable%: 0.2805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./lora_results\",\n",
        "    per_device_train_batch_size=1,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=30,\n",
        "    learning_rate=1e-4,\n",
        "    logging_steps=5,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    save_steps=30,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=lora_tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "id": "hsCChK1QsK49",
        "outputId": "b2c77de1-454e-4745-b5a0-81d3b2aea24f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 00:21, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>18.394275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>17.881631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>11.363309</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>47.866992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>7.219372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>32.055228</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=30, training_loss=22.463467915852863, metrics={'train_runtime': 24.4732, 'train_samples_per_second': 4.903, 'train_steps_per_second': 1.226, 'total_flos': 56012329451520.0, 'train_loss': 22.463467915852863, 'epoch': 0.6})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "gen = pipeline(\n",
        "    task=\"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "test_prompt = create_prompt(\n",
        "    context=\"Transformers use attention to model dependencies between tokens in a sequence.\",\n",
        "    question=\"What is attention used for?\",\n",
        "    answers={\"text\": [\"\"], \"answer_start\": [0]}\n",
        ")\n",
        "\n",
        "out = gen(test_prompt, max_new_tokens=40, do_sample=False)\n",
        "print(out[0][\"generated_text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4gMEqtasK22",
        "outputId": "ae8c23a0-64c3-45ad-a844-dfc5a2601516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Passing `generation_config` together with generation-related arguments=({'max_new_tokens', 'do_sample'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
            "Both `max_new_tokens` (=40) and `max_length`(=20) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Context: Transformers use attention to model dependencies between tokens in a sequence.\n",
            "### Question: What is attention used for?\n",
            "### Answer:  Attention is used to model dependencies between tokens in a sequence. The model is used to predict the next token in the sequence. The model is used to predict the next token in the sequence. The model\n"
          ]
        }
      ]
    }
  ]
}