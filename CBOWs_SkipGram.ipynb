{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDH-teacher/Deep_Learning_Audit_code/blob/main/CBOWs_SkipGram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVUv5WXilGpv",
        "outputId": "241d8e8b-a158-4e24-dafc-db630c33ff65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7910441fbd10>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# The code implementation is from https://janghan-kor.tistory.com/586\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CONTEXT_SIZE = 2  # 2 words to the left, 2 to the right\n",
        "text = \"\"\"We are about to study the idea of a computational process.\n",
        "Computational processes are abstract beings that inhabit computers.\n",
        "As they evolve, processes manipulate other abstract things called data.\n",
        "The evolution of a process is directed by a pattern of rules\n",
        "called a program. People create programs to direct processes. In effect,\n",
        "we conjure the spirits of the computer with our spells.\"\"\".split() ## This is our corpus\n",
        "\n",
        "# By deriving a set from `raw_text`, we deduplicate the array\n",
        "'''\n",
        "첫 번째 작업. vocavluary를 만듦. (0, 0, 0, 0, 0, 0, 0) 단어를 몇 번째 index만 1로 바꾸는 One-hot encoding을 사용.\n",
        "\n",
        "we = (1, 0, 0, 0, 0, 0, 0)\n",
        "are = (0, 1, 0, 0, 0, 0, 0)\n",
        "about = (0, 0, 1, 0, 0, 0, 0)\n",
        "to = (0, 0, 0, 1, 0, 0, 0)\n",
        "이렇게 저장하는 것은 힘듦.\n",
        "'''\n",
        "# text를 그냥 split하면 모든 것이 다 나옴.\n",
        "vocab = set(text) # set을 사용해서 vocalvulary로 만듦.\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "print('vocab_size:', vocab_size)\n",
        "\n",
        "\n",
        "# 앞에서부터 하나씩 배정\n",
        "w2i = {w: i for i, w in enumerate(vocab)}\n",
        "i2w = {i: w for i, w in enumerate(vocab)}\n",
        "\n",
        "# 자연어에서 가장 먼저하는 것이 이러한 vocalvulary를 만드는 것이다.\n",
        "print(w2i)\n",
        "print(i2w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3r8eeQDzlLKe",
        "outputId": "7fee1eed-ce87-4ada-ad46-0bb6e9d8ac0b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 49\n",
            "{'programs': 0, 'Computational': 1, 'that': 2, 'by': 3, 'People': 4, 'abstract': 5, 'of': 6, 'process': 7, 'the': 8, 'computational': 9, 'are': 10, 'evolution': 11, 'our': 12, 'idea': 13, 'a': 14, 'to': 15, 'manipulate': 16, 'things': 17, 'inhabit': 18, 'processes.': 19, 'As': 20, 'The': 21, 'rules': 22, 'about': 23, 'direct': 24, 'create': 25, 'conjure': 26, 'pattern': 27, 'program.': 28, 'We': 29, 'process.': 30, 'spells.': 31, 'computers.': 32, 'data.': 33, 'is': 34, 'processes': 35, 'evolve,': 36, 'we': 37, 'they': 38, 'spirits': 39, 'with': 40, 'directed': 41, 'called': 42, 'study': 43, 'other': 44, 'In': 45, 'effect,': 46, 'beings': 47, 'computer': 48}\n",
            "{0: 'programs', 1: 'Computational', 2: 'that', 3: 'by', 4: 'People', 5: 'abstract', 6: 'of', 7: 'process', 8: 'the', 9: 'computational', 10: 'are', 11: 'evolution', 12: 'our', 13: 'idea', 14: 'a', 15: 'to', 16: 'manipulate', 17: 'things', 18: 'inhabit', 19: 'processes.', 20: 'As', 21: 'The', 22: 'rules', 23: 'about', 24: 'direct', 25: 'create', 26: 'conjure', 27: 'pattern', 28: 'program.', 29: 'We', 30: 'process.', 31: 'spells.', 32: 'computers.', 33: 'data.', 34: 'is', 35: 'processes', 36: 'evolve,', 37: 'we', 38: 'they', 39: 'spirits', 40: 'with', 41: 'directed', 42: 'called', 43: 'study', 44: 'other', 45: 'In', 46: 'effect,', 47: 'beings', 48: 'computer'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# context window size is two\n",
        "\n",
        "\n",
        "# input : t-2, t-1, t+1, t+2\n",
        "# Output : t\n",
        "# 각 단어가 들어오면 목적으로 하는 Output의 값을 정하면 양쪽 2개씩의 값이 필요.\n",
        "\n",
        "def create_cbow_dataset(text):\n",
        "    data = []\n",
        "    for i in range(2, len(text) - 2): # 0번째는는 앞에에 두두 개개 없음음.\n",
        "        context = [text[i - 2], text[i - 1],\n",
        "                   text[i + 1], text[i + 2]]\n",
        "        target = text[i] # 나머지는 context, 현재 t번째를 target으로 지정해서 데이터를 만들겠다.\n",
        "        data.append((context, target))\n",
        "    return data\n",
        "\n",
        "'''\n",
        "우리는 input은 현재 단어, Output은 4개.\n",
        "\n",
        "보통 4개의 output으로 하지는 않고,\n",
        "t - > t-2\n",
        "t - > t-1\n",
        "t - > t + 1\n",
        "t - > t + 2\n",
        "입력 값에 대한 context를 학습.\n",
        "'''\n",
        "# input : t\n",
        "# Output : t-2, t-1, t+1, t+2\n",
        "\n",
        "def create_skipgram_dataset(text):\n",
        "    import random\n",
        "    data = []\n",
        "    for i in range(2, len(text) - 2):\n",
        "        data.append((text[i], text[i-2], 1))\n",
        "        data.append((text[i], text[i-1], 1))\n",
        "        data.append((text[i], text[i+1], 1))\n",
        "        data.append((text[i], text[i+2], 1))\n",
        "        # negative sampling\n",
        "        for _ in range(4):\n",
        "            if random.random() < 0.5 or i >= len(text) - 3:\n",
        "                rand_id = random.randint(0, i-1)\n",
        "            else:\n",
        "                rand_id = random.randint(i+3, len(text)-1)\n",
        "            data.append((text[i], text[rand_id], 0))\n",
        "    return data\n",
        "\n",
        "cbow_train = create_cbow_dataset(text)\n",
        "skipgram_train = create_skipgram_dataset(text)\n",
        "print('cbow sample', cbow_train[0])\n",
        "print('skipgram sample', skipgram_train[0])\n",
        "print(skipgram_train[-1])\n",
        "\n",
        "\n",
        "# ################################\n",
        "# =>We, about, he\n",
        "# [1,0,0] =>0\n",
        "# [0,1,0] =>1\n",
        "# [0,0,1] =>2\n",
        "\n",
        "\n",
        "# W =[ [0.1, 0.2, 0.3],\n",
        "#   [-0.4, 0.5, 0.6],\n",
        "#   [0.01, 0.02, 0.03]]\n",
        "\n",
        "# ([0,1,0], [1,0,0], 1)\n",
        "# ([1,0],1)\n",
        "# x = [1,0]=> model -> W\n",
        "# ==>Wx ==> [-0.4, 0.5, 0.6], [0.1, 0.2, 0.3]\n",
        "\n",
        "sentence =\"We are about to study\"\n",
        "\n",
        "'''\n",
        "\n",
        "window = 1\n",
        "\n",
        "[We, are, 1]\n",
        "[are, We, 1]\n",
        "[are, about, 1]\n",
        "[about, are, 1]\n",
        "[about, to, 1]\n",
        "[study, to, 1]\n",
        "\n",
        "[We, study, 0]\n",
        "[about, We, 0]\n",
        "\n",
        "W1 [5,3]\n",
        "W2 [5,3]\n",
        "\n",
        "\n",
        "W1 = [[0.1, 0.2, 0.3],\n",
        "      [0.4, 0.5, 0.6],\n",
        "      [0.7, 0.8, 0.9],\n",
        "      [...],\n",
        "      [...]]\n",
        "W2 = [[0.01, 0.02, 0.03],\n",
        "      [],\n",
        "      [],\n",
        "      [],\n",
        "      []]\n",
        "We : 0\n",
        "are: 1\n",
        "about: 2\n",
        "to : 3\n",
        "study :4\n",
        "\n",
        "\n",
        "[We, are, 1]\n",
        "\n",
        "[1,0,0,0,0] @ W1 => [0.1, 0.2, 0.3]\n",
        "[0] => nn.Embedding => [0.1,0.2,0.3]\n",
        "\n",
        "[We, are, about, to, study]\n",
        "[[1,0,0,0,0],\n",
        " [0,1,0,0,0],\n",
        " ..]\n",
        "\n",
        "[1,1,1,1,1]\n",
        "[0,1,2,3,4]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "[[are, about],We, 1]\n",
        "\n",
        "[0.4, 0.5, 0.6],\n",
        "[0.7, 0.8, 0.9]\n",
        "\n",
        "[0.14,0.33,0.25]\n",
        "\n",
        "[0.01, 0.02, 0.03]\n",
        "\n",
        "\n",
        "[We, are, 1]\n",
        "We: [0.1, 0.2, 0.3]\n",
        "\n",
        "\n",
        "[0.1, 0.2, 0.3] * W2\n",
        "|V| => [0.6, 0.3, 0.4, 0.2, 0.1] => [0,1,0,0,0]\n",
        "\n",
        "[we, about]\n",
        "[w, e]\n",
        "[ab, out, 1]\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "p504Wl4ElP6n",
        "outputId": "85261345-baac-4e1b-8e9a-23eff005f44c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cbow sample (['We', 'are', 'to', 'study'], 'about')\n",
            "skipgram sample ('about', 'We', 1)\n",
            "('with', 'rules', 0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nwindow = 1\\n\\n[We, are, 1]\\n[are, We, 1]\\n[are, about, 1]\\n[about, are, 1]\\n[about, to, 1]\\n[study, to, 1]\\n\\n[We, study, 0]\\n[about, We, 0]\\n\\nW1 [5,3]\\nW2 [5,3]\\n\\n\\nW1 = [[0.1, 0.2, 0.3],\\n      [0.4, 0.5, 0.6],\\n      [0.7, 0.8, 0.9],\\n      [...],\\n      [...]]\\nW2 = [[0.01, 0.02, 0.03],\\n      [],\\n      [],\\n      [],\\n      []]\\nWe : 0\\nare: 1\\nabout: 2\\nto : 3\\nstudy :4\\n\\n\\n[We, are, 1]\\n\\n[1,0,0,0,0] @ W1 => [0.1, 0.2, 0.3]\\n[0] => nn.Embedding => [0.1,0.2,0.3]\\n\\n[We, are, about, to, study]\\n[[1,0,0,0,0],\\n [0,1,0,0,0],\\n ..]\\n\\n[1,1,1,1,1]\\n[0,1,2,3,4]\\n\\n\\n\\n\\n[[are, about],We, 1]\\n\\n[0.4, 0.5, 0.6],\\n[0.7, 0.8, 0.9]\\n\\n[0.14,0.33,0.25]\\n\\n[0.01, 0.02, 0.03]\\n\\n\\n[We, are, 1]\\nWe: [0.1, 0.2, 0.3]\\n\\n\\n[0.1, 0.2, 0.3] * W2\\n|V| => [0.6, 0.3, 0.4, 0.2, 0.1] => [0,1,0,0,0]\\n\\n[we, about]\\n[w, e]\\n[ab, out, 1]\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CBOW(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size, context_size, hidden_size):\n",
        "        super(CBOW, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "        self.linear1 = nn.Linear(2*context_size*embd_size, hidden_size) # input이이 4개개 에에 대해해 hidden layer 지났다가가 감\n",
        "        self.linear2 = nn.Linear(hidden_size, vocab_size) # (0, 0, 0, 0, 0, 1, 0, 0) # 6번째째 인덱스를를 갖는 단어다.\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedded = self.embeddings(inputs).view((1, -1))\n",
        "        hid = F.relu(self.linear1(embedded))\n",
        "        out = self.linear2(hid)\n",
        "        log_probs = F.log_softmax(out)\n",
        "        return log_probs\n",
        "\n",
        "class SkipGram(nn.Module):\n",
        "    def __init__(self, vocab_size, embd_size):\n",
        "        super(SkipGram, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embd_size)\n",
        "        # [0]\n",
        "        # [1,0,0,0,0] W\n",
        "\n",
        "\n",
        "    def forward(self, focus, context):\n",
        "        embed_focus = self.embeddings(focus).view((1, -1))\n",
        "        embed_ctx = self.embeddings(context).view((1, -1))\n",
        "        score = torch.mm(embed_focus, torch.t(embed_ctx))\n",
        "        log_probs = F.logsigmoid(score)\n",
        "\n",
        "        return log_probs"
      ],
      "metadata": {
        "id": "P_RWLLPolUN5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embd_size = 100\n",
        "learning_rate = 0.001\n",
        "n_epoch = 30\n",
        "\n",
        "def train_cbow():\n",
        "    hidden_size = 64\n",
        "    losses = []\n",
        "    # NLLLoss + softmax function = cross-entropy loss (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html)\n",
        "    # https://discuss.pytorch.org/t/difference-between-cross-entropy-loss-or-log-likelihood-loss/38816/2\n",
        "    loss_fn = nn.NLLLoss()\n",
        "    model = CBOW(vocab_size, embd_size, CONTEXT_SIZE, hidden_size)\n",
        "    print(model)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss = .0\n",
        "        for context, target in cbow_train:\n",
        "            ctx_idxs = [w2i[w] for w in context]\n",
        "            ctx_var = Variable(torch.LongTensor(ctx_idxs))\n",
        "\n",
        "            model.zero_grad()\n",
        "            log_probs = model(ctx_var)\n",
        "\n",
        "            loss = loss_fn(log_probs, Variable(torch.LongTensor([w2i[target]])))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        losses.append(total_loss)\n",
        "    return model, losses\n",
        "\n",
        "def train_skipgram():\n",
        "    losses = []\n",
        "    loss_fn = nn.MSELoss()\n",
        "    model = SkipGram(vocab_size, embd_size)\n",
        "    print(model)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(n_epoch):\n",
        "        total_loss = .0\n",
        "        for in_w, out_w, target in skipgram_train:\n",
        "            in_w_var = Variable(torch.LongTensor([w2i[in_w]]))\n",
        "            out_w_var = Variable(torch.LongTensor([w2i[out_w]]))\n",
        "\n",
        "            model.zero_grad()\n",
        "            log_probs = model(in_w_var, out_w_var)\n",
        "            loss = loss_fn(log_probs[0], Variable(torch.Tensor([target])))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "        losses.append(total_loss)\n",
        "    return model, losses\n",
        "\n",
        "cbow_model, cbow_losses = train_cbow()\n",
        "sg_model, sg_losses = train_skipgram()\n"
      ],
      "metadata": {
        "id": "xD0sYUVulYW_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e6a2687-fdce-4258-f4b4-beb5c53a96a3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CBOW(\n",
            "  (embeddings): Embedding(49, 100)\n",
            "  (linear1): Linear(in_features=400, out_features=64, bias=True)\n",
            "  (linear2): Linear(in_features=64, out_features=49, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3938013615.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  log_probs = F.log_softmax(out)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SkipGram(\n",
            "  (embeddings): Embedding(49, 100)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test\n",
        "# You have to use other dataset for test, but in this case I use training data because this dataset is too small\n",
        "def test_cbow(test_data, model):\n",
        "    print('====Test CBOW===')\n",
        "    correct_ct = 0\n",
        "    for ctx, target in test_data:\n",
        "        ctx_idxs = [w2i[w] for w in ctx]\n",
        "        ctx_var = Variable(torch.LongTensor(ctx_idxs))\n",
        "\n",
        "        model.zero_grad()\n",
        "        log_probs = model(ctx_var)\n",
        "        _, predicted = torch.max(log_probs.data, 1)\n",
        "        predicted_word = i2w[predicted.item()] # predicted는 tensor임. 이것을을 어떠한한 값으로로 바꿔야함함. 그것이이 Item.\n",
        "        print('predicted:', predicted_word)\n",
        "        print('label    :', target)\n",
        "        if predicted_word == target:\n",
        "            correct_ct += 1\n",
        "\n",
        "    print('Accuracy: {:.1f}% ({:d}/{:d})'.format(correct_ct/len(test_data)*100, correct_ct, len(test_data)))\n",
        "\n",
        "def test_skipgram(test_data, model):\n",
        "    print('====Test SkipGram===')\n",
        "    correct_ct = 0\n",
        "    for in_w, out_w, target in test_data:\n",
        "        in_w_var = Variable(torch.LongTensor([w2i[in_w]]))\n",
        "        out_w_var = Variable(torch.LongTensor([w2i[out_w]]))\n",
        "\n",
        "        model.zero_grad()\n",
        "        log_probs = model(in_w_var, out_w_var)\n",
        "        _, predicted = torch.max(log_probs.data, 1)\n",
        "        predicted = predicted[0]\n",
        "        predicted_word = i2w[predicted.item()] # predicted는 tensor임. 이것을을 어떠한한 값으로로 바꿔야함함. 그것이이 Item.\n",
        "        if predicted == target:\n",
        "            correct_ct += 1\n",
        "\n",
        "    print('Accuracy: {:.1f}% ({:d}/{:d})'.format(correct_ct/len(test_data)*100, correct_ct, len(test_data)))\n",
        "\n",
        "test_cbow(cbow_train, cbow_model)\n",
        "print('------')\n",
        "test_skipgram(skipgram_train, sg_model)"
      ],
      "metadata": {
        "id": "9zNtCQnNlcSf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "046c84df-275f-49de-c6ec-4a556394091e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====Test CBOW===\n",
            "predicted: about\n",
            "label    : about\n",
            "predicted: to\n",
            "label    : to\n",
            "predicted: study\n",
            "label    : study\n",
            "predicted: the\n",
            "label    : the\n",
            "predicted: idea\n",
            "label    : idea\n",
            "predicted: of\n",
            "label    : of\n",
            "predicted: a\n",
            "label    : a\n",
            "predicted: computational\n",
            "label    : computational\n",
            "predicted: process.\n",
            "label    : process.\n",
            "predicted: Computational\n",
            "label    : Computational\n",
            "predicted: processes\n",
            "label    : processes\n",
            "predicted: are\n",
            "label    : are\n",
            "predicted: abstract\n",
            "label    : abstract\n",
            "predicted: beings\n",
            "label    : beings\n",
            "predicted: that\n",
            "label    : that\n",
            "predicted: inhabit\n",
            "label    : inhabit\n",
            "predicted: computers.\n",
            "label    : computers.\n",
            "predicted: As\n",
            "label    : As\n",
            "predicted: they\n",
            "label    : they\n",
            "predicted: evolve,\n",
            "label    : evolve,\n",
            "predicted: processes\n",
            "label    : processes\n",
            "predicted: manipulate\n",
            "label    : manipulate\n",
            "predicted: other\n",
            "label    : other\n",
            "predicted: abstract\n",
            "label    : abstract\n",
            "predicted: things\n",
            "label    : things\n",
            "predicted: called\n",
            "label    : called\n",
            "predicted: a\n",
            "label    : data.\n",
            "predicted: The\n",
            "label    : The\n",
            "predicted: evolution\n",
            "label    : evolution\n",
            "predicted: of\n",
            "label    : of\n",
            "predicted: a\n",
            "label    : a\n",
            "predicted: process\n",
            "label    : process\n",
            "predicted: is\n",
            "label    : is\n",
            "predicted: directed\n",
            "label    : directed\n",
            "predicted: by\n",
            "label    : by\n",
            "predicted: a\n",
            "label    : a\n",
            "predicted: pattern\n",
            "label    : pattern\n",
            "predicted: of\n",
            "label    : of\n",
            "predicted: rules\n",
            "label    : rules\n",
            "predicted: called\n",
            "label    : called\n",
            "predicted: a\n",
            "label    : a\n",
            "predicted: program.\n",
            "label    : program.\n",
            "predicted: People\n",
            "label    : People\n",
            "predicted: create\n",
            "label    : create\n",
            "predicted: programs\n",
            "label    : programs\n",
            "predicted: to\n",
            "label    : to\n",
            "predicted: direct\n",
            "label    : direct\n",
            "predicted: processes.\n",
            "label    : processes.\n",
            "predicted: In\n",
            "label    : In\n",
            "predicted: effect,\n",
            "label    : effect,\n",
            "predicted: we\n",
            "label    : we\n",
            "predicted: conjure\n",
            "label    : conjure\n",
            "predicted: the\n",
            "label    : the\n",
            "predicted: idea\n",
            "label    : spirits\n",
            "predicted: of\n",
            "label    : of\n",
            "predicted: the\n",
            "label    : the\n",
            "predicted: called\n",
            "label    : computer\n",
            "predicted: with\n",
            "label    : with\n",
            "Accuracy: 94.8% (55/58)\n",
            "------\n",
            "====Test SkipGram===\n",
            "Accuracy: 50.0% (232/464)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3938013615.py:12: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  log_probs = F.log_softmax(out)\n"
          ]
        }
      ]
    }
  ]
}