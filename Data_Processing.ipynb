{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDH-teacher/Deep_Learning_Audit_code/blob/main/Data_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tSiVjnSKBlcK",
        "outputId": "f700b78c-ed4d-4603-ea01-d996a2bd403a"
      },
      "source": [
        "import json\n",
        "\n",
        "## Collected Dataset\n",
        "data = [('hide new secretions from the parental units','0'), ('contains no wit , only labored gags','0'), ('that loves its characters and communicates something rather beautiful about human nature', \"1\"),('remains utterly satisfied to remain the same throughout ',\t\"0\"), ('on the worst revenge-of-the-nerds clichés the filmmakers could dredge up',\"0\"), (\"that 's far too tragic to merit such superficial treatment\",\"0\"), (\"demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small , personal film with an emotional wallop .\" ,\"1\"), (\"of saucy\" ,\"1\"), (\"a depressed fifteen-year-old 's suicidal poetry\" ,\"0\"),(\"are more deeply thought through than in most ` right-thinking ' films\" ,\"1\"),(\"goes to absurd lengths\",\"0\"),(\"for those moviegoers who complain that ` they do n't make movies like they used to anymore\" ,\"0\"),(\"the part where nothing 's happening , \",\"0\"),(\"saw how bad this movie was\" ,\"0\"),(\"lend some dignity to a dumb story\" ,\"0\"),(\"the greatest musicians\" ,\"1\"),(\"cold movie \",\"0\")]\n",
        "\n",
        "\n",
        "### Training data pre-processing\n",
        "texts, labels = [], []\n",
        "label2idx = {'0':0, '1':1}\n",
        "for item in data:\n",
        "  text = item[0]\n",
        "\n",
        "  if text[0] == ' ':\n",
        "    text = text[1:]\n",
        "  # additional preprocessing\n",
        "  text = text.replace(\"  \",\" \") ## Replace double space\n",
        "  text = text.replace(\",\", \"\") ## Replace comma to \"\"\n",
        "  text = text.lower()  ## Lower cases\n",
        "\n",
        "  label = label2idx[item[1]]\n",
        "\n",
        "  texts.append(text)\n",
        "  labels.append(label)\n",
        "\n",
        "print(\"*\"*50)\n",
        "print(\"Total number of datasets\")\n",
        "print(len(texts))\n",
        "print(len(labels))\n",
        "print(\"*\"*50)\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**************************************************\n",
            "Total number of datasets\n",
            "17\n",
            "17\n",
            "**************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### Split into train/dev/test sets using library\n",
        "from sklearn.model_selection import train_test_split\n",
        "### Wrute a code for collecting samples for each class\n",
        "pos,neg = [], []\n",
        "for a,b in zip(texts,labels):\n",
        "  if b == '1':\n",
        "    pos.append((a,b))\n",
        "  else:\n",
        "    neg.append((a,b))\n",
        "\n",
        "rest_texts, test_texts, rest_labels, test_labels = train_test_split(texts, labels, test_size=0.1, random_state=1)\n",
        "train_texts, dev_texts, train_labels, dev_labels = train_test_split(rest_texts, rest_labels, test_size=0.1, random_state=1)\n",
        "data_dict = {'train':{}, 'dev':{}, 'test':{}}\n",
        "data_dict[\"train\"][\"texts\"] = train_texts\n",
        "data_dict[\"dev\"][\"texts\"] = dev_texts\n",
        "data_dict[\"test\"][\"texts\"] = test_texts\n",
        "data_dict[\"train\"][\"labels\"] = train_labels\n",
        "data_dict[\"dev\"][\"labels\"] = dev_labels\n",
        "data_dict[\"test\"][\"labels\"] = test_labels\n",
        "\n",
        "print(\"Train Dataset Examples\")\n",
        "print(train_texts[:3])\n",
        "print(train_labels[:3])\n",
        "print(\"*\"*50)\n",
        "print(train_labels)\n",
        "print(dev_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZyzodaGovkH",
        "outputId": "602b2917-8165-458b-e7e7-3e7f0a48aabf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset Examples\n",
            "['lend some dignity to a dumb story', 'demonstrates that the director of such hollywood blockbusters as patriot games can still turn out a small  personal film with an emotional wallop .', \"are more deeply thought through than in most ` right-thinking ' films\"]\n",
            "[0, 1, 1]\n",
            "**************************************************\n",
            "[0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0]\n",
            "[0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4wmL0jRBw2_",
        "outputId": "aac60d02-6a9f-4f7e-cc18-cce2f9515b73"
      },
      "source": [
        "## Construct a vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "all_words = []\n",
        "for item in train_texts:\n",
        "  all_words += item.split()\n",
        "for item in dev_texts:\n",
        "  all_words += item.split()\n",
        "\n",
        "## Build a dictionary that maps words to integers\n",
        "counts = Counter(all_words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {'<pad>':0, \"<unk>\":1}\n",
        "vocab_to_int.update({word: ii for ii, word in enumerate(vocab,2)})\n",
        "print(vocab_to_int)\n",
        "print(len(vocab_to_int))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'<pad>': 0, '<unk>': 1, 'the': 2, 'to': 3, 'that': 4, 'a': 5, \"'s\": 6, 'of': 7, 'such': 8, '`': 9, 'they': 10, 'lend': 11, 'some': 12, 'dignity': 13, 'dumb': 14, 'story': 15, 'demonstrates': 16, 'director': 17, 'hollywood': 18, 'blockbusters': 19, 'as': 20, 'patriot': 21, 'games': 22, 'can': 23, 'still': 24, 'turn': 25, 'out': 26, 'small': 27, 'personal': 28, 'film': 29, 'with': 30, 'an': 31, 'emotional': 32, 'wallop': 33, '.': 34, 'are': 35, 'more': 36, 'deeply': 37, 'thought': 38, 'through': 39, 'than': 40, 'in': 41, 'most': 42, 'right-thinking': 43, \"'\": 44, 'films': 45, 'on': 46, 'worst': 47, 'revenge-of-the-nerds': 48, 'clichés': 49, 'filmmakers': 50, 'could': 51, 'dredge': 52, 'up': 53, 'loves': 54, 'its': 55, 'characters': 56, 'and': 57, 'communicates': 58, 'something': 59, 'rather': 60, 'beautiful': 61, 'about': 62, 'human': 63, 'nature': 64, 'for': 65, 'those': 66, 'moviegoers': 67, 'who': 68, 'complain': 69, 'do': 70, \"n't\": 71, 'make': 72, 'movies': 73, 'like': 74, 'used': 75, 'anymore': 76, 'saucy': 77, 'far': 78, 'too': 79, 'tragic': 80, 'merit': 81, 'superficial': 82, 'treatment': 83, 'greatest': 84, 'musicians': 85, 'cold': 86, 'movie': 87, 'part': 88, 'where': 89, 'nothing': 90, 'happening': 91, 'depressed': 92, 'fifteen-year-old': 93, 'suicidal': 94, 'poetry': 95, 'contains': 96, 'no': 97, 'wit': 98, 'only': 99, 'labored': 100, 'gags': 101, 'goes': 102, 'absurd': 103, 'lengths': 104, 'hide': 105, 'new': 106, 'secretions': 107, 'from': 108, 'parental': 109, 'units': 110}\n",
            "111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "def encode_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Encodes inputs\n",
        "    Returns input_ids, segment_ids, and attention_mask.\n",
        "    \"\"\"\n",
        "    max_length = 50\n",
        "    input_ids = []\n",
        "    for item in sentence.split():\n",
        "      if item in vocab_to_int:\n",
        "        input_ids.append(vocab_to_int[item])\n",
        "      else:\n",
        "        input_ids.append(vocab_to_int['<unk>'])\n",
        "    segment_ids = [0]*len(input_ids)\n",
        "    attention_mask = [1] * len(input_ids) #inputs['attention_mask']\n",
        "    padding_length = max_length - len(input_ids)\n",
        "    input_ids += [vocab_to_int['<pad>']] * padding_length\n",
        "    segment_ids += [0] * padding_length\n",
        "    attention_mask += [0] * padding_length\n",
        "    for input_elem in (input_ids, segment_ids, attention_mask):\n",
        "        assert len(input_elem) == max_length\n",
        "    return (\n",
        "        torch.tensor(input_ids).long(),\n",
        "        torch.tensor(segment_ids).long(),\n",
        "        torch.tensor(attention_mask).long(),\n",
        "    )\n",
        "\n",
        "def encode_label(label):\n",
        "    \"\"\"Wraps label in tensor.\"\"\"\n",
        "\n",
        "    return torch.tensor(label).long()\n",
        "\n"
      ],
      "metadata": {
        "id": "GJ0RsEBgqidq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxnDYmRRB1iP"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset wrapper. Used for storing, retrieving, encoding, caching, and batching samples.\n",
        "    \"\"\"\n",
        "    def __init__(self, sentences, labels):\n",
        "      ## Process text data (tokenization, encoding) and save this information into self.samples\n",
        "      self.sentences = sentences\n",
        "      self.labels = labels\n",
        "      self.cache = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        res = self.cache.get(i, None)\n",
        "        if res is None:\n",
        "          sentence = self.sentences[i]\n",
        "          label = self.labels[i]\n",
        "          input_ids, segment_ids, attention_mask = encode_sentence(sentence)\n",
        "          label_id = encode_label(label)\n",
        "          res = ((input_ids, segment_ids, attention_mask, label_id))\n",
        "          self.cache[i] = res\n",
        "          return res\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(train_texts, train_labels)\n",
        "print(f'train samples = {len(train_dataset)}')\n",
        "dev_dataset = TextDataset(dev_texts, dev_labels)\n",
        "print(f'dev samples = {len(dev_dataset)}')\n",
        "test_dataset = TextDataset(test_texts, test_labels)\n",
        "print(f'test samples = {len(test_dataset)}')\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, 8, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, 8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, 8, shuffle=True)\n",
        "\n",
        "for item in dev_dataset:\n",
        "  print(item)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rgKz8DJqojw",
        "outputId": "a80c55cc-5063-4e42-8810-d8710bbdf785"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train samples = 13\n",
            "dev samples = 2\n",
            "test samples = 2\n",
            "(tensor([102,   3, 103, 104,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]), tensor([1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]), tensor(0))\n",
            "(tensor([105, 106, 107, 108,   2, 109, 110,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0]), tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]), tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0]), tensor(0))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JGV2QbYD0U6k",
        "outputId": "16f69e11-076f-44bc-a2a7-d87ce6aeaa78"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset wrapper. Used for storing, retrieving, encoding, caching, and batching samples.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,path,processor):\n",
        "      ## Process text data (tokenization, encoding) and save this information into self.samples\n",
        "       self.samples = processor.load_samples(path)\n",
        "       self.cache = {}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "       res = self.cache.get(i, None)\n",
        "       if res is None:\n",
        "         sample = self.samples[i]\n",
        "         sentence, label = sample\n",
        "         input_ids, segment_ids, attention_mask = encode_sentence(sentence)\n",
        "         label_id = encode_label(label)\n",
        "         res = ((input_ids, segment_ids, attention_mask, label_id))\n",
        "       self.cache[i] = res\n",
        "       return res\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UxeVtd2z1JdX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "from tqdm import tqdm\n",
        "class SST2Processor:\n",
        "    \"\"\"Data loader for SST-2.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.label_map = {'0': 0, '1': 1}\n",
        "\n",
        "    def valid_inputs(self, sentence1, label):\n",
        "        return len(sentence1) > 0 and label in self.label_map\n",
        "\n",
        "    def load_samples(self, path,train=True):\n",
        "        samples = []\n",
        "        with open(path, newline='') as f:\n",
        "            reader = csv.reader(f, delimiter='\\t')\n",
        "            next(reader)  # skip header\n",
        "            desc = f'loading \\'{path}\\''\n",
        "            for row in tqdm(reader, desc=desc):\n",
        "                sentence = row[0]\n",
        "                label = row[1]\n",
        "                if self.valid_inputs(sentence, label):\n",
        "                    label = self.label_map[label]\n",
        "                    samples.append((sentence, label))\n",
        "        return samples\n",
        "\n"
      ],
      "metadata": {
        "id": "OnWxddIV1mwn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = SST2Processor()\n",
        "train_dataset = TextDataset(\"/content/gdrive/MyDrive/data/SST2/train.tsv\", processor)\n",
        "print(f'train samples = {len(train_dataset)}')\n",
        "dev_dataset = TextDataset(\"/content/gdrive/MyDrive/data/SST2/dev.tsv\", processor)\n",
        "print(f'dev samples = {len(dev_dataset)}')\n",
        "test_dataset = TextDataset(\"/content/gdrive/MyDrive/data/SST2/test.tsv\", processor)\n",
        "print(f'test samples = {len(test_dataset)}')\n",
        "\n",
        "for item in dev_dataset:\n",
        "  print(item)"
      ],
      "metadata": {
        "id": "3X9F3k-T1qUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_sentence(sentence):\n",
        "    \"\"\"\n",
        "    Encodes inputs\n",
        "    Returns input_ids, segment_ids, and attention_mask.\n",
        "    \"\"\"\n",
        "    max_length = 256\n",
        "\n",
        "    from transformers import AutoTokenizer\n",
        "    BERT_MODEL = 'bert-base-uncased'\n",
        "    tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
        "\n",
        "    inputs = tokenizer.encode(\n",
        "        sentence, max_length=max_length\n",
        "    )\n",
        "    input_ids = inputs\n",
        "    segment_ids = [0]*len(inputs)\n",
        "    attention_mask = [1] * len(inputs) #inputs['attention_mask']\n",
        "    padding_length = max_length - len(input_ids)\n",
        "    input_ids += [tokenizer.pad_token_id] * padding_length\n",
        "    segment_ids += [0] * padding_length\n",
        "    attention_mask += [0] * padding_length\n",
        "    for input_elem in (input_ids, segment_ids, attention_mask):\n",
        "        assert len(input_elem) == max_length\n",
        "    return (\n",
        "        cuda(torch.tensor(input_ids).long()),\n",
        "        cuda(torch.tensor(segment_ids).long()),\n",
        "        cuda(torch.tensor(attention_mask).long()),\n",
        "    )\n",
        "\n",
        "def encode_label(label):\n",
        "    \"\"\"Wraps label in tensor.\"\"\"\n",
        "\n",
        "    return cuda(torch.tensor(label)).long()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RMMVxtc93C3V"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cuda(tensor):\n",
        "    \"\"\"Places tensor on CUDA device.\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "      return tensor.cuda()\n",
        "    else:\n",
        "      return tensor"
      ],
      "metadata": {
        "id": "3FuKR_q_3r9w"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor = SST2Processor()\n",
        "train_dataset = TextDataset(\"/content/gdrive/MyDrive/data/SST2/train.tsv\", processor)\n",
        "print(f'train samples = {len(train_dataset)}')\n",
        "dev_dataset = TextDataset(\"/content/gdrive/MyDrive/data/SST2/dev.tsv\", processor)\n",
        "print(f'dev samples = {len(dev_dataset)}')\n",
        "test_dataset = TextDataset(\"/content/gdrive/MyDrive/data/SST2/test.tsv\", processor)\n",
        "print(f'test samples = {len(test_dataset)}')\n",
        "\n",
        "for item in dev_dataset:\n",
        "  print(item)\n"
      ],
      "metadata": {
        "id": "nacpGXLS3ElS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define RNN model\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        # Define Embedding\n",
        "        self.embedding = nn.Embedding(len(vocab_to_int), input_size, padding_idx=vocab_to_int['<pad>'])\n",
        "\n",
        "        # Define the RNN layer\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
        "\n",
        "        # Define the fully connected layer to produce outputs\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedding = self.embedding(x)\n",
        "        # Forward propagate the RNN\n",
        "        out, hidden = self.rnn(embedding)\n",
        "        # Take the output from the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "# Example usage\n",
        "input_size = 10  # Number of input features per time step\n",
        "hidden_size = 20 # Number of features in the hidden state\n",
        "num_layers = 2   # Number of RNN layers (stacked)\n",
        "output_size = 1  # Number of output classes\n",
        "\n",
        "# Create RNN model\n",
        "model = RNNModel(input_size, hidden_size, num_layers, output_size)\n"
      ],
      "metadata": {
        "id": "p_9Ak6sl66f1"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TextDataset(train_texts, train_labels)\n",
        "print(f'train samples = {len(train_dataset)}')\n",
        "dev_dataset = TextDataset(dev_texts, dev_labels)\n",
        "print(f'dev samples = {len(dev_dataset)}')\n",
        "test_dataset = TextDataset(test_texts, test_labels)\n",
        "print(f'test samples = {len(test_dataset)}')\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train_dataset, 8, shuffle=True)\n",
        "dev_loader = DataLoader(dev_dataset, 8, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, 8, shuffle=True)\n",
        "\n",
        "##Train API\n",
        "for train_data in train_loader:\n",
        "  input_ids, attention_masks, segment_ids, labels = train_data\n",
        "  out = model(input_ids)\n",
        "  print(out.shape)\n",
        "  ## Define a loss..\n",
        "  ## Backpropagation..\n",
        "\n"
      ],
      "metadata": {
        "id": "-5IIqkBd6-Z_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}