{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO1Z0IQ2rCwlBSDtyb47FLF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDH-teacher/Deep_Learning_Audit_code/blob/main/GPT_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iFqxq3Ykv1B",
        "outputId": "f28f58e7-31dd-4fec-e73c-e0d2b6857008"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ],
      "source": [
        "!pip -q install transformers datasets evaluate accelerate\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import torch\n",
        "from transformers import (\n",
        "    GPT2Tokenizer,\n",
        "    GPT2LMHeadModel,\n",
        "    GPT2ForSequenceClassification,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "from torch.utils.data import Dataset, random_split\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) GPT-2: Autoregressive Text Generation (without fine-tuning)\n",
        "\n",
        "# beam search / top-p / top-k :contentReference[oaicite:1]{index=1}\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "gen_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "gen_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# GPT-2 tokenizer는 pad_token이 기본적으로 없음 → eos를 pad로 사용 :contentReference[oaicite:2]{index=2}\n",
        "gen_tokenizer.pad_token = gen_tokenizer.eos_token\n",
        "gen_model.config.pad_token_id = gen_tokenizer.eos_token_id\n",
        "\n",
        "prompt = \"The future of artificial intelligence is\"\n",
        "input_ids = gen_tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# (1) Beam Search\n",
        "beam_out = gen_model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    num_beams=5,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True\n",
        ")\n",
        "print(\"\\n[Beam]\")\n",
        "print(gen_tokenizer.decode(beam_out[0], skip_special_tokens=True))\n",
        "\n",
        "# (2) Top-p sampling\n",
        "topp_out = gen_model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    top_p=0.9,\n",
        "    top_k=0,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    do_sample=True\n",
        ")\n",
        "print(\"\\n[Top-p]\")\n",
        "print(gen_tokenizer.decode(topp_out[0], skip_special_tokens=True))\n",
        "\n",
        "# (3) Top-k sampling\n",
        "topk_out = gen_model.generate(\n",
        "    input_ids,\n",
        "    max_length=50,\n",
        "    top_k=50,\n",
        "    top_p=1.0,\n",
        "    no_repeat_ngram_size=2,\n",
        "    early_stopping=True,\n",
        "    do_sample=True\n",
        ")\n",
        "print(\"\\n[Top-k]\")\n",
        "print(gen_tokenizer.decode(topk_out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbUhaweHlo43",
        "outputId": "fd579578-25ca-4b6b-cdb5-543d9f3dfa82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Beam]\n",
            "The future of artificial intelligence is in the hands of the next generation of scientists and engineers.\n",
            "\n",
            "\"It's a very exciting time to be a part of it,\" he said. \"I think it's going to take a lot of work to\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Top-p]\n",
            "The future of artificial intelligence is defined by what we've done. We're trying to create AI technology that is smarter than humans, and get that sort of same result. It will probably be something like an optogenetic autonomous sensor or smart house.\n",
            "\n",
            "[Top-k]\n",
            "The future of artificial intelligence is uncertain. Even as some AI is being developed, researchers are debating whether to put its advances into machine learning or human-computer interaction, according to Richard Kurzweil, the chief of MIT's computer science degree program\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) GPT-2: Fine-tuning (Autoregressive) on \"contradicted sentences\"\n",
        "\n",
        "# 데이터(contradicted sentences) :contentReference[oaicite:3]{index=3}\n",
        "# GPT-2 로드/데이터셋/Trainer 흐름 :contentReference[oaicite:4]{index=4}\n",
        "\n",
        "ft_texts = [\n",
        "    \"The sky is always blue, except when it’s completely gray.\",\n",
        "    \"I always tell the truth, but I just lied to you.\",\n",
        "    \"The cat was outside in the rain, and it was completely dry.\",\n",
        "    \"He is both the fastest runner and the slowest.\",\n",
        "    \"She never forgets, except for today.\",\n",
        "    \"I’m both awake and asleep at the same time.\",\n",
        "    \"He was definitely here earlier, but now he's nowhere to be found.\",\n",
        "    \"She’s a vegetarian who loves eating steak.\",\n",
        "    \"This is the best book I’ve ever read, but I wouldn’t recommend it.\",\n",
        "    \"It’s so hot today that I need a jacket.\",\n",
        "    \"The concert was amazing, but the singer couldn’t carry a tune.\",\n",
        "    \"I’m hungry, but I can’t eat right now.\",\n",
        "    \"She was walking, but somehow she didn’t move at all.\",\n",
        "    \"I just can’t wait to go home, but I never want to leave.\",\n",
        "    \"They’re both professional athletes and couch potatoes.\",\n",
        "    \"The light was so bright that I couldn’t see anything.\",\n",
        "    \"He never makes mistakes, except for today.\",\n",
        "    \"It’s freezing cold, and yet I’m sweating.\",\n",
        "    \"I can hear the sound of silence, and it’s deafening.\",\n",
        "    \"She’s always the life of the party, but she hates crowds.\",\n",
        "    \"I’m not going anywhere, but I’m packing my bags.\",\n",
        "    \"The room is so quiet, it feels like a concert.\",\n",
        "    \"I can't speak French, but I can say 'Bonjour' perfectly.\",\n",
        "]\n",
        "\n",
        "ft_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "ft_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "ft_tokenizer.pad_token = ft_tokenizer.eos_token\n",
        "ft_model.config.pad_token_id = ft_tokenizer.eos_token_id\n",
        "\n",
        "class GPT2LMTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data[idx]\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = enc[\"input_ids\"].squeeze(0)\n",
        "        attention_mask = enc[\"attention_mask\"].squeeze(0)\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100  # pad는 loss에서 무시\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "ft_dataset = GPT2LMTextDataset(ft_texts, ft_tokenizer, max_length=128)\n",
        "train_size = int(0.6 * len(ft_dataset))\n",
        "dev_size = int(0.2 * len(ft_dataset))\n",
        "test_size = len(ft_dataset) - train_size - dev_size\n",
        "train_ds, dev_ds, test_ds = random_split(ft_dataset, [train_size, dev_size, test_size])\n",
        "\n",
        "ft_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_ft_results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,          # 1 epoch\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "ft_trainer = Trainer(\n",
        "    model=ft_model,\n",
        "    args=ft_args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=dev_ds,\n",
        ")\n",
        "\n",
        "ft_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "63JSPcGslsLx",
        "outputId": "9d9e2f18-c9ef-480a-8f07-e8fd059167c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='7' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [7/7 00:01, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>3.555832</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=7, training_loss=3.821747371128627, metrics={'train_runtime': 1.7041, 'train_samples_per_second': 7.629, 'train_steps_per_second': 4.108, 'total_flos': 849199104000.0, 'train_loss': 3.821747371128627, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 저장 + 생성 함수 흐름 :contentReference[oaicite:5]{index=5}\n",
        "ft_model.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "ft_tokenizer.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "\n",
        "def autoregressive_generation(model, tokenizer, input_text):\n",
        "    model.eval()\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(model.device)\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        max_length=50,\n",
        "        num_return_sequences=1,\n",
        "        no_repeat_ngram_size=2,\n",
        "        do_sample=True,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        temperature=0.7\n",
        "    )\n",
        "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n[Fine-tuned generation sample]\")\n",
        "print(autoregressive_generation(ft_model, ft_tokenizer, \"I am hungry, but\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "XBis5YxZlzrY",
        "outputId": "6c83d756-d3c3-495a-fb66-2aea96f2c78d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Fine-tuned generation sample]\n",
            "I am hungry, but I am not going to be the one who eats.\n",
            "\n",
            "\"I have to work for something, so I need to live my life. I have the money, and I live with my family. It's not easy\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=5.5843305587768555, metrics={'train_runtime': 0.3338, 'train_samples_per_second': 8.988, 'train_steps_per_second': 5.992, 'total_flos': 195969024000.0, 'train_loss': 5.5843305587768555, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3) GPT-2로 Seq2Seq 흉내내기: [SEP] 추가 + \"input [SEP] output\" 포맷\n",
        "\n",
        "# [SEP] 추가/resize + Dataset + Trainer + inference :contentReference[oaicite:6]{index=6}\n",
        "\n",
        "sep_token = \"[SEP]\"\n",
        "\n",
        "s2s_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "s2s_model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
        "\n",
        "s2s_tokenizer.pad_token = s2s_tokenizer.eos_token\n",
        "s2s_model.config.pad_token_id = s2s_tokenizer.eos_token_id\n",
        "\n",
        "s2s_tokenizer.add_special_tokens({\"additional_special_tokens\": [sep_token]})\n",
        "s2s_model.resize_token_embeddings(len(s2s_tokenizer))\n",
        "\n",
        "\n",
        "texts = [\n",
        "    \"Translate English to French: Hello, how are you?\",\n",
        "    \"Translate English to French: I love robotics.\",\n",
        "    \"Translate English to French: This course is about pre-trained language models.\",\n",
        "]\n",
        "targets = [\n",
        "    \"Bonjour, comment ça va ?\",\n",
        "    \"J'aime la robotique.\",\n",
        "    \"Ce cours porte sur les modèles de langage pré-entraînés.\",\n",
        "]\n",
        "\n",
        "class Seq2SeqDataset(Dataset):\n",
        "    def __init__(self, tokenizer, texts, targets, max_length=128):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = texts\n",
        "        self.targets = targets\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        input_text = self.texts[idx]\n",
        "        target_text = self.targets[idx]\n",
        "        formatted_text = f\"{input_text} {sep_token} {target_text}\"\n",
        "\n",
        "        enc = self.tokenizer(\n",
        "            formatted_text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = enc[\"input_ids\"].squeeze()\n",
        "        attention_mask = enc[\"attention_mask\"].squeeze()\n",
        "\n",
        "        labels = input_ids.clone()\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "s2s_dataset = Seq2SeqDataset(s2s_tokenizer, texts, targets, max_length=128)\n",
        "\n",
        "s2s_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_s2s_results\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=1,                # 1 epoch\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "s2s_trainer = Trainer(\n",
        "    model=s2s_model,\n",
        "    args=s2s_args,\n",
        "    train_dataset=s2s_dataset,\n",
        ")\n",
        "\n",
        "s2s_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "H5zRHezSl8yf",
        "outputId": "80997191-a562-40a6-e174-5b67f1384886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=5.584334373474121, metrics={'train_runtime': 0.286, 'train_samples_per_second': 10.49, 'train_steps_per_second': 6.993, 'total_flos': 195969024000.0, 'train_loss': 5.584334373474121, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_seq2seq(model, tokenizer, input_text, max_length=64):\n",
        "    model.eval()\n",
        "    prompt = f\"{input_text} {sep_token}\"\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "    output_ids = model.generate(input_ids, max_length=max_length, num_beams=5, early_stopping=True)\n",
        "    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    # [SEP] 이후를 답으로 간주\n",
        "    return output_text.split(sep_token)[-1].strip()\n",
        "\n",
        "print(\"\\n[GPT-2 Seq2Seq-style inference]\")\n",
        "test_input = \"Translate English to French: Hello, how are you?\"\n",
        "print(\"IN :\", test_input)\n",
        "print(\"OUT:\", generate_seq2seq(s2s_model, s2s_tokenizer, test_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xb9iyjG8l6YZ",
        "outputId": "033a3d38-2ddb-4cd7-a2d0-1cf4c0e7edc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[GPT-2 Seq2Seq-style inference]\n",
            "IN : Translate English to French: Hello, how are you?\n",
            "OUT: Translate English to French: Hello, how are you? \n",
            "\n",
            "Hello, how are you? Hello, how are you? Hello, how are you? Hello, how are you? Hello, how are you? Hello, how are you? Hello, how are you? Hello, how are you?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) GPT-2: Text Classification (GPT2ForSequenceClassification)\n",
        "# GPT-2는 pad_token 설정 필요 + GPT2ForSequenceClassification 사용\n",
        "\n",
        "# GPT2ForSequenceClassification 로드 + pad_token 설정 :contentReference[oaicite:7]{index=7}\n",
        "\n",
        "clf_tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "clf_model = GPT2ForSequenceClassification.from_pretrained(model_name, num_labels=2).to(device)\n",
        "\n",
        "clf_tokenizer.pad_token = clf_tokenizer.eos_token\n",
        "clf_model.config.pad_token_id = clf_tokenizer.eos_token_id\n",
        "\n",
        "# 분류 데이터(감정 이진 분류처럼)\n",
        "clf_data = [\n",
        "    (\"I love this paper. It is very helpful.\", 1),\n",
        "    (\"This is terrible and confusing.\", 0),\n",
        "    (\"Amazing explanation and great examples.\", 1),\n",
        "    (\"I dislike this lecture. It is boring.\", 0),\n",
        "    (\"Very clear and concise.\", 1),\n",
        "    (\"I cannot understand anything here.\", 0),\n",
        "]\n",
        "\n",
        "class ClfDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_length=128):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.data[idx]\n",
        "        enc = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
        "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "clf_dataset = ClfDataset(clf_data, clf_tokenizer, max_length=64)\n",
        "train_size = int(0.8 * len(clf_dataset))\n",
        "eval_size = len(clf_dataset) - train_size\n",
        "clf_train, clf_eval = random_split(clf_dataset, [train_size, eval_size])\n",
        "\n",
        "clf_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_clf_results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=2,\n",
        "    per_device_eval_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"no\",\n",
        "    report_to=[],\n",
        ")\n",
        "\n",
        "# padding 포함 배치를 깔끔하게 처리\n",
        "data_collator = DataCollatorWithPadding(tokenizer=clf_tokenizer)\n",
        "\n",
        "clf_trainer = Trainer(\n",
        "    model=clf_model,\n",
        "    args=clf_args,\n",
        "    train_dataset=clf_train,\n",
        "    eval_dataset=clf_eval,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "clf_trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "h_8AmyW-mCwE",
        "outputId": "a3401605-f4f9-488b-86e9-c1794d68889a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 88f33c03-db93-4c49-af15-1d60b0842d9d)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/config.json\n",
            "WARNING:huggingface_hub.utils._http:'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 88f33c03-db93-4c49-af15-1d60b0842d9d)')' thrown while requesting HEAD https://huggingface.co/gpt2/resolve/main/config.json\n",
            "Retrying in 1s [Retry 1/5].\n",
            "WARNING:huggingface_hub.utils._http:Retrying in 1s [Retry 1/5].\n",
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2/2 00:00, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.000360</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2, training_loss=5.450051307678223, metrics={'train_runtime': 0.2834, 'train_samples_per_second': 14.112, 'train_steps_per_second': 7.056, 'total_flos': 130648375296.0, 'train_loss': 5.450051307678223, 'epoch': 1.0})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_label(text: str):\n",
        "    clf_model.eval()\n",
        "    enc = clf_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        logits = clf_model(**enc).logits\n",
        "    pred = torch.argmax(logits, dim=-1).item()\n",
        "    return pred, logits.squeeze().cpu().tolist()\n",
        "\n",
        "print(\"\\n[GPT-2 classification inference]\")\n",
        "sample = \"This lecture is really helpful and clear.\"\n",
        "pred, logits = predict_label(sample)\n",
        "print(\"text :\", sample)\n",
        "print(\"pred :\", pred, \"(1=positive, 0=negative)\")\n",
        "print(\"logits:\", logits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GRVPacEmkC3",
        "outputId": "a6c080b7-903e-4e7f-accb-0ccd3247bd2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[GPT-2 classification inference]\n",
            "text : This lecture is really helpful and clear.\n",
            "pred : 0 (1=positive, 0=negative)\n",
            "logits: [9.733610153198242, 1.224496841430664]\n"
          ]
        }
      ]
    }
  ]
}