{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTGhFse18d6haq92HegyE4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BDH-teacher/Deep_Learning_Audit_code/blob/main/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwCLE2XEKCsP",
        "outputId": "ae110795-0aeb-4707-b5f2-4c6a7497751f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device = cuda\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import Transformer\n",
        "\n",
        "# (선택) 재현성\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"device =\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        # Create Position Vectors Matrix\n",
        "        self.pe = torch.zeros(max_len, d_model)  # First, initialize it as zeros\n",
        "        position = torch.arange(0, max_len).unsqueeze(1)  # [[1],[2],[3],[4],...max_len]\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        self.pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        self.pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = self.pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, seq_len, d_model) when batch_first=True\n",
        "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "id": "DV2wFXjGKHYO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        src_vocab_size,\n",
        "        tgt_vocab_size,\n",
        "        d_model=512,\n",
        "        nhead=8,\n",
        "        num_encoder_layers=3,\n",
        "        num_decoder_layers=3,\n",
        "        dim_feedforward=2048,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_encoder_layers,\n",
        "            num_decoder_layers=num_decoder_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.d_model = d_model\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        tgt,\n",
        "        src_mask=None,\n",
        "        tgt_mask=None,\n",
        "        memory_mask=None,\n",
        "        src_key_padding_mask=None,\n",
        "        tgt_key_padding_mask=None,\n",
        "        memory_key_padding_mask=None\n",
        "    ):\n",
        "        src = self.positional_encoding(self.src_embedding(src) * math.sqrt(self.d_model))\n",
        "        tgt = self.positional_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
        "\n",
        "        memory = self.transformer.encoder(\n",
        "            src,\n",
        "            mask=src_mask,\n",
        "            src_key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "\n",
        "        output = self.transformer.decoder(\n",
        "            tgt,\n",
        "            memory,\n",
        "            tgt_mask=tgt_mask,\n",
        "            memory_mask=memory_mask,\n",
        "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "            memory_key_padding_mask=memory_key_padding_mask\n",
        "        )\n",
        "\n",
        "        return self.fc_out(output)"
      ],
      "metadata": {
        "id": "awep6PRoKHWW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_square_subsequent_mask(size):\n",
        "    mask = (torch.triu(torch.ones(size, size)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def make_key_padding_mask(x, pad_idx=0):\n",
        "    # x: (batch, seq_len)\n",
        "    # True = padding 위치 (무시됨)\n",
        "    return (x == pad_idx)"
      ],
      "metadata": {
        "id": "_zEMJrWbKHUH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Small sample dataset (English to French)\n",
        "data = [\n",
        "    (\"I love you\", \"Je t'aime\"),\n",
        "    (\"I am learning\", \"J'apprends\"),\n",
        "    (\"How are you\", \"Comment ca va\"),\n",
        "    (\"Thank you\", \"Merci\"),\n",
        "    (\"Good morning\", \"Bonjour\"),\n",
        "]\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        vocab.update(sentence.split())\n",
        "\n",
        "    vocab = sorted(list(vocab))\n",
        "    vocab_dict = {word: idx for idx, word in enumerate(vocab, start=4)}  # Reserve 0-3 for special tokens\n",
        "    vocab_dict[\"<pad>\"] = 0\n",
        "    vocab_dict[\"<sos>\"] = 1\n",
        "    vocab_dict[\"<eos>\"] = 2\n",
        "    vocab_dict[\"<unk>\"] = 3\n",
        "    return vocab_dict\n",
        "\n",
        "def sentence_to_indices(sentence, vocab, max_len=10):\n",
        "    tokens = sentence.split()\n",
        "    indices = [vocab.get(token, vocab[\"<unk>\"]) for token in tokens]\n",
        "    indices = [vocab[\"<sos>\"]] + indices + [vocab[\"<eos>\"]]\n",
        "    # Padding to max_len\n",
        "    if len(indices) < max_len:\n",
        "        indices = indices + [vocab[\"<pad>\"]] * (max_len - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_len]\n",
        "    return indices\n",
        "\n",
        "src_sentences, tgt_sentences = zip(*data)\n",
        "src_vocab = build_vocab(src_sentences)\n",
        "tgt_vocab = build_vocab(tgt_sentences)\n",
        "\n",
        "max_len = 10\n",
        "src_data = torch.tensor([sentence_to_indices(s, src_vocab, max_len=max_len) for s in src_sentences], dtype=torch.long)\n",
        "tgt_data = torch.tensor([sentence_to_indices(s, tgt_vocab, max_len=max_len) for s in tgt_sentences], dtype=torch.long)\n",
        "\n",
        "print(\"src_data shape:\", src_data.shape)  # (N, max_len)\n",
        "print(\"tgt_data shape:\", tgt_data.shape)\n",
        "\n",
        "# id -> token (번역 출력용)\n",
        "tgt_id_to_word = {idx: w for w, idx in tgt_vocab.items()}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv4gOXZBKOFt",
        "outputId": "cce9cd14-3cce-4aad-b126-30aa92c5d102"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src_data shape: torch.Size([5, 10])\n",
            "tgt_data shape: torch.Size([5, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = len(src_vocab)\n",
        "tgt_vocab_size = len(tgt_vocab)\n",
        "\n",
        "d_model = 128\n",
        "nhead = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "dropout = 0.0  # <- 정규화 싫으면 0.0\n",
        "\n",
        "model = Seq2SeqTransformer(\n",
        "    src_vocab_size, tgt_vocab_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    num_decoder_layers=num_decoder_layers,\n",
        "    dim_feedforward=512,\n",
        "    dropout=dropout\n",
        ")\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss(ignore_index=src_vocab[\"<pad>\"])  # pad token 무시"
      ],
      "metadata": {
        "id": "-AbEzdUXKOD2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, optimizer, loss_fn, src_data, tgt_data, epochs=200, print_every=20, pad_idx=0):\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for src, tgt in zip(src_data, tgt_data):\n",
        "            # Add batch dimension\n",
        "            src = src.unsqueeze(0)  # (1, max_len)\n",
        "            tgt = tgt.unsqueeze(0)  # (1, max_len)\n",
        "\n",
        "            # tgt_input: [<sos> ...] / tgt_output: [... <eos>]\n",
        "            tgt_input = tgt[:, :-1]\n",
        "            tgt_output = tgt[:, 1:]\n",
        "\n",
        "            # Masks\n",
        "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(src.device)\n",
        "\n",
        "            src_key_padding_mask = make_key_padding_mask(src, pad_idx=pad_idx)\n",
        "            tgt_key_padding_mask = make_key_padding_mask(tgt_input, pad_idx=pad_idx)\n",
        "            memory_key_padding_mask = make_key_padding_mask(src, pad_idx=pad_idx)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(\n",
        "                src,\n",
        "                tgt_input,\n",
        "                tgt_mask=tgt_mask,\n",
        "                src_key_padding_mask=src_key_padding_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "\n",
        "            # (batch, tgt_len, vocab) -> (batch*tgt_len, vocab)\n",
        "            output = output.reshape(-1, output.shape[-1])\n",
        "            tgt_output = tgt_output.reshape(-1)\n",
        "\n",
        "            loss = loss_fn(output, tgt_output)\n",
        "            loss.backward()\n",
        "\n",
        "            # (NaN 방지용) grad clip\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(src_data)\n",
        "        if epoch % print_every == 0 or epoch == 1:\n",
        "            print(f\"Epoch {epoch:4d}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# 실행\n",
        "train_epoch(\n",
        "    model, optimizer, loss_fn,\n",
        "    src_data, tgt_data,\n",
        "    epochs=200, print_every=20,\n",
        "    pad_idx=src_vocab[\"<pad>\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JFkJZWgNKOB4",
        "outputId": "7a05d5d9-d1f4-4e10-a817-c497669dc441"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/functional.py:6044: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch    1/200, Loss: 2.7530\n",
            "Epoch   20/200, Loss: 0.0036\n",
            "Epoch   40/200, Loss: 0.0016\n",
            "Epoch   60/200, Loss: 0.0009\n",
            "Epoch   80/200, Loss: 0.0006\n",
            "Epoch  100/200, Loss: 0.0005\n",
            "Epoch  120/200, Loss: 0.0003\n",
            "Epoch  140/200, Loss: 0.0003\n",
            "Epoch  160/200, Loss: 0.0002\n",
            "Epoch  180/200, Loss: 0.0002\n",
            "Epoch  200/200, Loss: 0.0001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence, model, src_vocab, tgt_vocab, tgt_id_to_word, max_len=10):\n",
        "    model.eval()\n",
        "\n",
        "    # Preprocess the input sentence\n",
        "    indices = [src_vocab.get(token, src_vocab[\"<unk>\"]) for token in sentence.split()]\n",
        "    indices = [src_vocab[\"<sos>\"]] + indices + [src_vocab[\"<eos>\"]]\n",
        "    if len(indices) < max_len:\n",
        "        indices = indices + [src_vocab[\"<pad>\"]] * (max_len - len(indices))\n",
        "    else:\n",
        "        indices = indices[:max_len]\n",
        "\n",
        "    src_tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "    # Generate target sequence with the start token\n",
        "    tgt_input = torch.tensor([tgt_vocab[\"<sos>\"]], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_len):\n",
        "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(src_tensor.device)\n",
        "\n",
        "            src_key_padding_mask = make_key_padding_mask(src_tensor, pad_idx=src_vocab[\"<pad>\"])\n",
        "            tgt_key_padding_mask = make_key_padding_mask(tgt_input, pad_idx=tgt_vocab[\"<pad>\"])\n",
        "            memory_key_padding_mask = make_key_padding_mask(src_tensor, pad_idx=src_vocab[\"<pad>\"])\n",
        "\n",
        "            output = model(\n",
        "                src_tensor,\n",
        "                tgt_input,\n",
        "                tgt_mask=tgt_mask,\n",
        "                src_key_padding_mask=src_key_padding_mask,\n",
        "                tgt_key_padding_mask=tgt_key_padding_mask,\n",
        "                memory_key_padding_mask=memory_key_padding_mask\n",
        "            )\n",
        "\n",
        "            next_token = output[:, -1, :].argmax(dim=-1)  # Get the predicted token\n",
        "            tgt_input = torch.cat((tgt_input, next_token.unsqueeze(0)), dim=1)  # Append\n",
        "\n",
        "            if next_token.item() == tgt_vocab[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "    # Convert predicted indices to words (exclude pad/sos/eos)\n",
        "    pred_ids = tgt_input.squeeze(0).tolist()\n",
        "    words = []\n",
        "    for idx in pred_ids:\n",
        "        if idx in (tgt_vocab[\"<pad>\"], tgt_vocab[\"<sos>\"], tgt_vocab[\"<eos>\"]):\n",
        "            continue\n",
        "        words.append(tgt_id_to_word.get(idx, \"<unk>\"))\n",
        "\n",
        "    return \" \".join(words)\n",
        "\n",
        "# 간단 테스트\n",
        "tests = [\n",
        "    \"I love you\",\n",
        "    \"Thank you\",\n",
        "    \"Good morning\",\n",
        "    \"How are you\",\n",
        "    \"I am learning\",\n",
        "]\n",
        "\n",
        "for s in tests:\n",
        "    print(f\"[SRC] {s}\")\n",
        "    print(f\"[PRED] {translate(s, model, src_vocab, tgt_vocab, tgt_id_to_word, max_len=max_len)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gyn7IbxKKN6X",
        "outputId": "d43afd39-da3a-4ee6-8aea-8e818eb97909"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SRC] I love you\n",
            "[PRED] Je t'aime\n",
            "--------------------------------------------------\n",
            "[SRC] Thank you\n",
            "[PRED] Merci\n",
            "--------------------------------------------------\n",
            "[SRC] Good morning\n",
            "[PRED] Bonjour\n",
            "--------------------------------------------------\n",
            "[SRC] How are you\n",
            "[PRED] Comment ca va\n",
            "--------------------------------------------------\n",
            "[SRC] I am learning\n",
            "[PRED] J'apprends\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
            "  output = torch._nested_tensor_from_mask(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RNN Seq2Seq (GRU)\n",
        "class RNNEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, hid_dim=128):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "\n",
        "    def forward(self, src):  # src: (src_len, batch)\n",
        "        emb = self.emb(src)  # (src_len, batch, emb_dim)\n",
        "        outputs, hidden = self.rnn(emb)  # hidden: (1, batch, hid_dim)\n",
        "        return hidden\n",
        "\n",
        "class RNNDecoder(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=64, hid_dim=128):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim, hid_dim)\n",
        "        self.fc = nn.Linear(hid_dim, vocab_size)\n",
        "\n",
        "    def forward(self, tgt_inp, hidden):  # tgt_inp: (tgt_len, batch)\n",
        "        emb = self.emb(tgt_inp)\n",
        "        outputs, hidden = self.rnn(emb, hidden)\n",
        "        logits = self.fc(outputs)  # (tgt_len, batch, vocab)\n",
        "        return logits, hidden\n",
        "\n",
        "class RNNSeq2Seq(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, emb_dim=64, hid_dim=128):\n",
        "        super().__init__()\n",
        "        self.encoder = RNNEncoder(src_vocab_size, emb_dim, hid_dim)\n",
        "        self.decoder = RNNDecoder(tgt_vocab_size, emb_dim, hid_dim)\n",
        "\n",
        "    def forward(self, src_indices, tgt_input):\n",
        "        # src_indices: (src_len, batch)\n",
        "        # tgt_input: (tgt_len, batch)  (teacher forcing input)\n",
        "        hidden = self.encoder(src_indices)\n",
        "        outputs, _ = self.decoder(tgt_input, hidden)\n",
        "        return outputs\n",
        "\n",
        "# ----- 슬라이드 스타일 vocab: <bos>/<eos>/<unk> -----\n",
        "def build_vocab_bos_eos(sentences):\n",
        "    vocab = {\"<pad>\": 0, \"<bos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n",
        "    idx = 4\n",
        "    for s in sentences:\n",
        "        for w in s.split():\n",
        "            if w not in vocab:\n",
        "                vocab[w] = idx\n",
        "                idx += 1\n",
        "    return vocab\n",
        "\n",
        "pairs = data  # (src, tgt)\n",
        "src_vocab2 = build_vocab_bos_eos([p[0] for p in pairs])\n",
        "tgt_vocab2 = build_vocab_bos_eos([p[1] for p in pairs])\n",
        "\n",
        "rnn_model = RNNSeq2Seq(len(src_vocab2), len(tgt_vocab2))\n",
        "\n",
        "# RNN based Seq2Seq train API\n",
        "def train(model, pairs, src_vocab, tgt_vocab, epochs=10, lr=0.01):\n",
        "    model.train()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "\n",
        "        for src, tgt in pairs:\n",
        "            src_indices = torch.tensor(\n",
        "                [src_vocab[\"<bos>\"]] + [src_vocab.get(word, src_vocab[\"<unk>\"]) for word in src.split()] + [src_vocab[\"<eos>\"]],\n",
        "                dtype=torch.long\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "            tgt_indices = torch.tensor(\n",
        "                [tgt_vocab[\"<bos>\"]] + [tgt_vocab.get(word, tgt_vocab[\"<unk>\"]) for word in tgt.split()] + [tgt_vocab[\"<eos>\"]],\n",
        "                dtype=torch.long\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(src_indices, tgt_indices[:-1])\n",
        "\n",
        "            loss = criterion(outputs.view(-1, len(tgt_vocab)), tgt_indices[1:].view(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(pairs):.4f}\")\n",
        "\n",
        "# 실행(옵션)\n",
        "train(rnn_model, pairs, src_vocab2, tgt_vocab2, epochs=30, lr=0.01)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWGbxd51KHR3",
        "outputId": "53242ea8-3d4b-45b8-c2c3-575533f4c65d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 2.5393\n",
            "Epoch 2/30, Loss: 1.1826\n",
            "Epoch 3/30, Loss: 0.4234\n",
            "Epoch 4/30, Loss: 0.1564\n",
            "Epoch 5/30, Loss: 0.0628\n",
            "Epoch 6/30, Loss: 0.0195\n",
            "Epoch 7/30, Loss: 0.0089\n",
            "Epoch 8/30, Loss: 0.0052\n",
            "Epoch 9/30, Loss: 0.0036\n",
            "Epoch 10/30, Loss: 0.0027\n",
            "Epoch 11/30, Loss: 0.0022\n",
            "Epoch 12/30, Loss: 0.0019\n",
            "Epoch 13/30, Loss: 0.0017\n",
            "Epoch 14/30, Loss: 0.0015\n",
            "Epoch 15/30, Loss: 0.0014\n",
            "Epoch 16/30, Loss: 0.0013\n",
            "Epoch 17/30, Loss: 0.0012\n",
            "Epoch 18/30, Loss: 0.0012\n",
            "Epoch 19/30, Loss: 0.0011\n",
            "Epoch 20/30, Loss: 0.0010\n",
            "Epoch 21/30, Loss: 0.0010\n",
            "Epoch 22/30, Loss: 0.0010\n",
            "Epoch 23/30, Loss: 0.0009\n",
            "Epoch 24/30, Loss: 0.0009\n",
            "Epoch 25/30, Loss: 0.0008\n",
            "Epoch 26/30, Loss: 0.0008\n",
            "Epoch 27/30, Loss: 0.0008\n",
            "Epoch 28/30, Loss: 0.0007\n",
            "Epoch 29/30, Loss: 0.0007\n",
            "Epoch 30/30, Loss: 0.0007\n"
          ]
        }
      ]
    }
  ]
}